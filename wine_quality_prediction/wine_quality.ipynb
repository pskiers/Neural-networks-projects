{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set loading and initial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.44875459371171905\n",
      "5 0.2974683544303797\n",
      "7 0.17966516945692118\n",
      "8 0.03572886892609228\n",
      "4 0.03327888934258881\n",
      "3 0.004083299305839118\n",
      "9 0.0010208248264597796\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.407735</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>-0.613832</td>\n",
       "      <td>1.192840</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>0.745247</td>\n",
       "      <td>0.949902</td>\n",
       "      <td>1.144836</td>\n",
       "      <td>-0.579315</td>\n",
       "      <td>0.254688</td>\n",
       "      <td>-1.233702</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>-0.292636</td>\n",
       "      <td>-0.090870</td>\n",
       "      <td>-0.359803</td>\n",
       "      <td>-0.215637</td>\n",
       "      <td>-0.031692</td>\n",
       "      <td>1.271472</td>\n",
       "      <td>1.184434</td>\n",
       "      <td>0.422593</td>\n",
       "      <td>0.798633</td>\n",
       "      <td>2.597109</td>\n",
       "      <td>-0.590190</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>-0.059179</td>\n",
       "      <td>1.918303</td>\n",
       "      <td>-0.021097</td>\n",
       "      <td>-0.479726</td>\n",
       "      <td>0.062218</td>\n",
       "      <td>-1.125776</td>\n",
       "      <td>-1.348520</td>\n",
       "      <td>-1.356752</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.341444</td>\n",
       "      <td>2.305611</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>-1.343192</td>\n",
       "      <td>0.399173</td>\n",
       "      <td>0.402285</td>\n",
       "      <td>-0.323229</td>\n",
       "      <td>-0.595151</td>\n",
       "      <td>-0.716490</td>\n",
       "      <td>-1.043627</td>\n",
       "      <td>-0.989065</td>\n",
       "      <td>1.520415</td>\n",
       "      <td>-0.612875</td>\n",
       "      <td>1.018588</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>0.407735</td>\n",
       "      <td>0.791206</td>\n",
       "      <td>0.232932</td>\n",
       "      <td>-0.137388</td>\n",
       "      <td>-0.360376</td>\n",
       "      <td>-0.541081</td>\n",
       "      <td>-0.949814</td>\n",
       "      <td>-0.857748</td>\n",
       "      <td>-1.694797</td>\n",
       "      <td>0.861982</td>\n",
       "      <td>1.581661</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.876278</td>\n",
       "      <td>-0.090870</td>\n",
       "      <td>0.825667</td>\n",
       "      <td>0.214731</td>\n",
       "      <td>0.156127</td>\n",
       "      <td>1.739228</td>\n",
       "      <td>2.450911</td>\n",
       "      <td>0.553910</td>\n",
       "      <td>-0.448082</td>\n",
       "      <td>-0.179094</td>\n",
       "      <td>-1.233702</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>-0.292636</td>\n",
       "      <td>-1.070954</td>\n",
       "      <td>0.148256</td>\n",
       "      <td>-0.743816</td>\n",
       "      <td>-0.736016</td>\n",
       "      <td>-0.131795</td>\n",
       "      <td>-1.418879</td>\n",
       "      <td>-0.838050</td>\n",
       "      <td>0.208084</td>\n",
       "      <td>1.989815</td>\n",
       "      <td>0.616394</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>-0.292636</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>0.740991</td>\n",
       "      <td>0.351667</td>\n",
       "      <td>-0.078647</td>\n",
       "      <td>1.446880</td>\n",
       "      <td>1.348607</td>\n",
       "      <td>0.527646</td>\n",
       "      <td>-0.316849</td>\n",
       "      <td>-0.092337</td>\n",
       "      <td>-0.911946</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>-1.109735</td>\n",
       "      <td>-0.384895</td>\n",
       "      <td>-0.275126</td>\n",
       "      <td>-0.861189</td>\n",
       "      <td>-0.595151</td>\n",
       "      <td>-0.424142</td>\n",
       "      <td>-1.090534</td>\n",
       "      <td>-0.578699</td>\n",
       "      <td>1.323566</td>\n",
       "      <td>1.729546</td>\n",
       "      <td>0.294638</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0.757921</td>\n",
       "      <td>0.987223</td>\n",
       "      <td>1.418403</td>\n",
       "      <td>1.251527</td>\n",
       "      <td>-0.172557</td>\n",
       "      <td>1.271472</td>\n",
       "      <td>1.067168</td>\n",
       "      <td>1.440299</td>\n",
       "      <td>-0.644932</td>\n",
       "      <td>-0.265850</td>\n",
       "      <td>-1.233702</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2938 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "381        0.407735          0.007139    -0.613832        1.192840   0.015263   \n",
       "2073      -0.292636         -0.090870    -0.359803       -0.215637  -0.031692   \n",
       "2883      -0.059179          1.918303    -0.021097       -0.479726   0.062218   \n",
       "679       -1.343192          0.399173     0.402285       -0.323229  -0.595151   \n",
       "1915       0.407735          0.791206     0.232932       -0.137388  -0.360376   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "91        -0.876278         -0.090870     0.825667        0.214731   0.156127   \n",
       "3084      -0.292636         -1.070954     0.148256       -0.743816  -0.736016   \n",
       "3289      -0.292636          0.007139     0.740991        0.351667  -0.078647   \n",
       "793       -1.109735         -0.384895    -0.275126       -0.861189  -0.595151   \n",
       "1979       0.757921          0.987223     1.418403        1.251527  -0.172557   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "381              0.745247              0.949902  1.144836 -0.579315   \n",
       "2073             1.271472              1.184434  0.422593  0.798633   \n",
       "2883            -1.125776             -1.348520 -1.356752  0.273700   \n",
       "679             -0.716490             -1.043627 -0.989065  1.520415   \n",
       "1915            -0.541081             -0.949814 -0.857748 -1.694797   \n",
       "...                   ...                   ...       ...       ...   \n",
       "91               1.739228              2.450911  0.553910 -0.448082   \n",
       "3084            -0.131795             -1.418879 -0.838050  0.208084   \n",
       "3289             1.446880              1.348607  0.527646 -0.316849   \n",
       "793             -0.424142             -1.090534 -0.578699  1.323566   \n",
       "1979             1.271472              1.067168  1.440299 -0.644932   \n",
       "\n",
       "      sulphates   alcohol   quality  \n",
       "381    0.254688 -1.233702  0.500000  \n",
       "2073   2.597109 -0.590190  0.333333  \n",
       "2883   0.341444  2.305611  0.666667  \n",
       "679   -0.612875  1.018588  0.666667  \n",
       "1915   0.861982  1.581661  0.666667  \n",
       "...         ...       ...       ...  \n",
       "91    -0.179094 -1.233702  0.333333  \n",
       "3084   1.989815  0.616394  0.500000  \n",
       "3289  -0.092337 -0.911946  0.333333  \n",
       "793    1.729546  0.294638  0.666667  \n",
       "1979  -0.265850 -1.233702  0.500000  \n",
       "\n",
       "[2938 rows x 12 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', delimiter=\";\")\n",
    "\n",
    "for val in df[\"quality\"].unique():\n",
    "    print(val, len(df.loc[df[\"quality\"] == val])/len(df))\n",
    "\n",
    "min_quality = 3\n",
    "max_quality = 9\n",
    "\n",
    "# one_hot = pd.get_dummies(df[\"quality\"])\n",
    "# df = df.drop(\"quality\", axis=1)\n",
    "# df = df.join(one_hot)\n",
    "\n",
    "df[\"quality\"] = (df[\"quality\"] - min_quality) / (max_quality - min_quality)\n",
    "\n",
    "features_cols = df.columns[:-1]\n",
    "\n",
    "train = df.sample(frac=0.8, random_state=213)\n",
    "test = df.drop(train.index)\n",
    "valid = train.sample(frac=0.25, random_state=123)\n",
    "train = train.drop(valid.index)\n",
    "\n",
    "mean_value = train[features_cols].mean()\n",
    "std_value = train[features_cols].std()\n",
    "min_value = train[features_cols].min()\n",
    "max_value = train[features_cols].max()\n",
    "\n",
    "# standard scaling\n",
    "train[features_cols] = (train[features_cols] - mean_value) / std_value\n",
    "valid[features_cols] = (valid[features_cols] - mean_value) / std_value\n",
    "test[features_cols] = (test[features_cols] - mean_value) / std_value\n",
    "\n",
    "# min-max scaling\n",
    "# train[features_cols] = (train[features_cols] - min_value) / (max_value - min_value)\n",
    "# valid[features_cols] = (valid[features_cols] - min_value) / (max_value - min_value)\n",
    "# test[features_cols] = (test[features_cols] - min_value) / (max_value - min_value)\n",
    "\n",
    "# tanh estimator\n",
    "# train[features_cols] = 0.5 * (np.tanh(0.01 * (train[features_cols] - mean_value) / std_value) + 1)\n",
    "# valid[features_cols] = 0.5 * (np.tanh(0.01 * (valid[features_cols] - mean_value) / std_value) + 1)\n",
    "# test[features_cols] = 0.5 * (np.tanh(0.01 * (test[features_cols] - mean_value) / std_value) + 1)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = data.TensorDataset(torch.from_numpy(train.values[:,:-7]),torch.from_numpy(train.values[:,-7:]))\n",
    "# valid_dataset = data.TensorDataset(torch.from_numpy(valid.values[:,:-7]),torch.from_numpy(valid.values[:,-7:]))\n",
    "# test_dataset = data.TensorDataset(torch.from_numpy(test.values[:,:-7]),torch.from_numpy(test.values[:,-7:]))\n",
    "\n",
    "train_dataset = data.TensorDataset(torch.from_numpy(train.values[:,:-1]),torch.from_numpy(train.values[:,-1]))\n",
    "valid_dataset = data.TensorDataset(torch.from_numpy(valid.values[:,:-1]),torch.from_numpy(valid.values[:,-1]))\n",
    "test_dataset = data.TensorDataset(torch.from_numpy(test.values[:,:-1]),torch.from_numpy(test.values[:,-1]))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_dataloader = data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMultiLayerNet(nn.Module):\n",
    "    \"\"\"\n",
    "    What the name says\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            output_size: int,\n",
    "            activation: Callable | None = None,\n",
    "            output_func: Callable | None = None,\n",
    "            layers: list[int | float] | None = None\n",
    "        ) -> None:\n",
    "        super().__init__(\n",
    "\n",
    "        )\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.last_layer: nn.Module\n",
    "        self.activation: callable = activation\n",
    "        self.output_func: callable = output_func\n",
    "\n",
    "        in_size = input_size\n",
    "        for layer_size in layers:\n",
    "            if layer_size >= 1:\n",
    "                self.layers.append(nn.Linear(in_features=in_size, out_features=layer_size))\n",
    "                in_size = layer_size\n",
    "            else:\n",
    "                self.layers.append(nn.Dropout(layer_size))\n",
    "        self.last_layer = nn.Linear(in_features=in_size, out_features=output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = x.float()\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                out = self.activation(out)\n",
    "        out = self.last_layer(out)\n",
    "        return self.output_func(out) if self.output_func is not None else out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Class responsible for training the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            optim: torch.optim.Optimizer,\n",
    "            loss: Callable,\n",
    "            train_dataloader: data.DataLoader,\n",
    "            valid_dataloader: data.DataLoader,\n",
    "            test_dataloader: data.DataLoader,\n",
    "            metric: Callable,\n",
    "            verbose: bool = True\n",
    "        ):\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.optim = optim\n",
    "        self.loss = loss\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.metric = metric\n",
    "        self.verbose = verbose\n",
    "        self.history: list[tuple[float, float, float, float]] = []\n",
    "\n",
    "\n",
    "    def evaluate(self, eval_dataloader: data.DataLoader) -> float:\n",
    "        \"\"\"\n",
    "        Evlauates the model on given dataset\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        metric_scores = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for features, labels in eval_dataloader:\n",
    "                features = features.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predictions = self.model(features).squeeze(dim=1)\n",
    "\n",
    "                loss = self.loss(predictions, labels.float())\n",
    "                losses.append(loss.item())\n",
    "                metric_scores.append(self.metric(predictions, labels))\n",
    "        return sum(losses) / len(losses), sum(metric_scores) / len(metric_scores)\n",
    "\n",
    "    \n",
    "    def train_one_epoch(self) -> None:\n",
    "        \"\"\"\n",
    "        Trains one epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        metric_scores = []\n",
    "        self.model.train()\n",
    "        for features, labels in self.train_dataloader:\n",
    "            features = features.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            predictions = self.model(features).squeeze(dim=1)\n",
    "\n",
    "            loss = self.loss(predictions, labels.float())\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optim.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            metric_scores.append(self.metric(predictions, labels))\n",
    "        self.history.append(\n",
    "            (sum(losses)/len(losses), sum(metric_scores)/len(metric_scores), *self.evaluate(self.valid_dataloader))\n",
    "        )\n",
    "    \n",
    "    def train(self, epochs: int, early_stoping: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Trains model\n",
    "        \"\"\"\n",
    "        min_loss = None\n",
    "        without_progress = 0\n",
    "        try:\n",
    "            for epoch in range(epochs):\n",
    "                self.train_one_epoch()\n",
    "                if self.verbose is True:\n",
    "                    train_loss, train_metric, valid_loss, valid_metric = self.history[-1]\n",
    "                    print(\n",
    "                        f\"Epoch: {epoch}\\t Train loss: {train_loss:.4}\\t accuracy: {train_metric:.4}\\t Validation loss: {valid_loss:.4}\\t accuracy: {valid_metric:.4}\"\n",
    "                    )\n",
    "                if early_stoping > 0:\n",
    "                    _, _, valid_loss, _ = self.history[-1]\n",
    "                    if min_loss is None:\n",
    "                        min_loss = valid_loss\n",
    "                    elif min_loss > valid_loss:\n",
    "                        min_loss = valid_loss\n",
    "                        without_progress = 0\n",
    "                    else:\n",
    "                        without_progress += 1\n",
    "                    \n",
    "                    if without_progress >= early_stoping:\n",
    "                        break\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        if self.verbose is True:\n",
    "            test_loss, test_accuracy = self.evaluate(self.test_dataloader)\n",
    "            print(f\"Test loss: {test_loss:.4}\\t accuracy: {test_accuracy:.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    return float(torch.sum(torch.argmax(predictions, dim=1) == torch.argmax(labels, dim=1)) / len(labels))\n",
    "\n",
    "def mse_accuracy(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    min_val = 3\n",
    "    max_val = 9\n",
    "    return float(torch.sum(torch.round(predictions * (max_val - min_val)) == labels * (max_val - min_val)) / len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Train loss: 0.06926\t accuracy: 0.2706\t Validation loss: 0.02418\t accuracy: 0.4096\n",
      "Epoch: 1\t Train loss: 0.02669\t accuracy: 0.4098\t Validation loss: 0.02138\t accuracy: 0.4453\n",
      "Epoch: 2\t Train loss: 0.02119\t accuracy: 0.4439\t Validation loss: 0.0203\t accuracy: 0.4591\n",
      "Epoch: 3\t Train loss: 0.01935\t accuracy: 0.4826\t Validation loss: 0.01729\t accuracy: 0.5075\n",
      "Epoch: 4\t Train loss: 0.01877\t accuracy: 0.473\t Validation loss: 0.01765\t accuracy: 0.4977\n",
      "Epoch: 5\t Train loss: 0.01827\t accuracy: 0.4947\t Validation loss: 0.01799\t accuracy: 0.5031\n",
      "Epoch: 6\t Train loss: 0.01743\t accuracy: 0.4964\t Validation loss: 0.0154\t accuracy: 0.5251\n",
      "Epoch: 7\t Train loss: 0.01674\t accuracy: 0.5067\t Validation loss: 0.01873\t accuracy: 0.4972\n",
      "Epoch: 8\t Train loss: 0.01664\t accuracy: 0.5085\t Validation loss: 0.01492\t accuracy: 0.5491\n",
      "Epoch: 9\t Train loss: 0.01641\t accuracy: 0.5156\t Validation loss: 0.01452\t accuracy: 0.5437\n",
      "Epoch: 10\t Train loss: 0.01572\t accuracy: 0.5082\t Validation loss: 0.01655\t accuracy: 0.5236\n",
      "Epoch: 11\t Train loss: 0.01532\t accuracy: 0.5263\t Validation loss: 0.01548\t accuracy: 0.553\n",
      "Epoch: 12\t Train loss: 0.01505\t accuracy: 0.511\t Validation loss: 0.01519\t accuracy: 0.5568\n",
      "Epoch: 13\t Train loss: 0.01443\t accuracy: 0.5355\t Validation loss: 0.01456\t accuracy: 0.5564\n",
      "Epoch: 14\t Train loss: 0.0147\t accuracy: 0.5352\t Validation loss: 0.01439\t accuracy: 0.5618\n",
      "Epoch: 15\t Train loss: 0.01495\t accuracy: 0.5281\t Validation loss: 0.01697\t accuracy: 0.5168\n",
      "Epoch: 16\t Train loss: 0.01462\t accuracy: 0.5298\t Validation loss: 0.01699\t accuracy: 0.528\n",
      "Epoch: 17\t Train loss: 0.01419\t accuracy: 0.5373\t Validation loss: 0.01489\t accuracy: 0.5564\n",
      "Epoch: 18\t Train loss: 0.014\t accuracy: 0.5487\t Validation loss: 0.01509\t accuracy: 0.55\n",
      "Epoch: 19\t Train loss: 0.01362\t accuracy: 0.5533\t Validation loss: 0.01412\t accuracy: 0.5652\n",
      "Epoch: 20\t Train loss: 0.01359\t accuracy: 0.5437\t Validation loss: 0.01397\t accuracy: 0.5764\n",
      "Epoch: 21\t Train loss: 0.01311\t accuracy: 0.5515\t Validation loss: 0.01484\t accuracy: 0.5559\n",
      "Epoch: 22\t Train loss: 0.01336\t accuracy: 0.5494\t Validation loss: 0.01387\t accuracy: 0.5598\n",
      "Epoch: 23\t Train loss: 0.0136\t accuracy: 0.5501\t Validation loss: 0.01419\t accuracy: 0.5587\n",
      "Epoch: 24\t Train loss: 0.0132\t accuracy: 0.5597\t Validation loss: 0.0141\t accuracy: 0.5607\n",
      "Epoch: 25\t Train loss: 0.01344\t accuracy: 0.5685\t Validation loss: 0.0145\t accuracy: 0.5515\n",
      "Epoch: 26\t Train loss: 0.01295\t accuracy: 0.5479\t Validation loss: 0.01385\t accuracy: 0.5691\n",
      "Epoch: 27\t Train loss: 0.01295\t accuracy: 0.5657\t Validation loss: 0.01462\t accuracy: 0.5603\n",
      "Epoch: 28\t Train loss: 0.01283\t accuracy: 0.549\t Validation loss: 0.01369\t accuracy: 0.5416\n",
      "Epoch: 29\t Train loss: 0.01304\t accuracy: 0.571\t Validation loss: 0.01382\t accuracy: 0.5832\n",
      "Epoch: 30\t Train loss: 0.01255\t accuracy: 0.5589\t Validation loss: 0.0136\t accuracy: 0.572\n",
      "Epoch: 31\t Train loss: 0.0123\t accuracy: 0.5717\t Validation loss: 0.013\t accuracy: 0.5696\n",
      "Epoch: 32\t Train loss: 0.01276\t accuracy: 0.555\t Validation loss: 0.01404\t accuracy: 0.5612\n",
      "Epoch: 33\t Train loss: 0.01279\t accuracy: 0.5653\t Validation loss: 0.01475\t accuracy: 0.549\n",
      "Epoch: 34\t Train loss: 0.01276\t accuracy: 0.565\t Validation loss: 0.01483\t accuracy: 0.549\n",
      "Epoch: 35\t Train loss: 0.01233\t accuracy: 0.5707\t Validation loss: 0.01366\t accuracy: 0.5729\n",
      "Epoch: 36\t Train loss: 0.01257\t accuracy: 0.5671\t Validation loss: 0.0133\t accuracy: 0.5426\n",
      "Epoch: 37\t Train loss: 0.01218\t accuracy: 0.57\t Validation loss: 0.01406\t accuracy: 0.5544\n",
      "Epoch: 38\t Train loss: 0.01223\t accuracy: 0.5732\t Validation loss: 0.01368\t accuracy: 0.5666\n",
      "Epoch: 39\t Train loss: 0.01209\t accuracy: 0.5703\t Validation loss: 0.01357\t accuracy: 0.5592\n",
      "Epoch: 40\t Train loss: 0.01185\t accuracy: 0.5767\t Validation loss: 0.01407\t accuracy: 0.548\n",
      "Epoch: 41\t Train loss: 0.01217\t accuracy: 0.5803\t Validation loss: 0.01403\t accuracy: 0.5578\n",
      "Epoch: 42\t Train loss: 0.01214\t accuracy: 0.5721\t Validation loss: 0.01312\t accuracy: 0.5725\n",
      "Epoch: 43\t Train loss: 0.01209\t accuracy: 0.5632\t Validation loss: 0.01343\t accuracy: 0.5671\n",
      "Epoch: 44\t Train loss: 0.01147\t accuracy: 0.5863\t Validation loss: 0.01365\t accuracy: 0.572\n",
      "Epoch: 45\t Train loss: 0.01163\t accuracy: 0.5778\t Validation loss: 0.01382\t accuracy: 0.5618\n",
      "Epoch: 46\t Train loss: 0.012\t accuracy: 0.5831\t Validation loss: 0.01324\t accuracy: 0.5563\n",
      "Epoch: 47\t Train loss: 0.01175\t accuracy: 0.5806\t Validation loss: 0.0138\t accuracy: 0.5642\n",
      "Epoch: 48\t Train loss: 0.01195\t accuracy: 0.5845\t Validation loss: 0.01317\t accuracy: 0.5573\n",
      "Epoch: 49\t Train loss: 0.01172\t accuracy: 0.5721\t Validation loss: 0.01385\t accuracy: 0.5672\n",
      "Epoch: 50\t Train loss: 0.01179\t accuracy: 0.5756\t Validation loss: 0.01452\t accuracy: 0.553\n",
      "Epoch: 51\t Train loss: 0.0119\t accuracy: 0.5888\t Validation loss: 0.01348\t accuracy: 0.5814\n",
      "Epoch: 52\t Train loss: 0.01181\t accuracy: 0.5863\t Validation loss: 0.01361\t accuracy: 0.573\n",
      "Epoch: 53\t Train loss: 0.01153\t accuracy: 0.5955\t Validation loss: 0.01348\t accuracy: 0.5715\n",
      "Epoch: 54\t Train loss: 0.01142\t accuracy: 0.5806\t Validation loss: 0.01319\t accuracy: 0.5685\n",
      "Epoch: 55\t Train loss: 0.01165\t accuracy: 0.5881\t Validation loss: 0.0138\t accuracy: 0.5646\n",
      "Epoch: 56\t Train loss: 0.01123\t accuracy: 0.5831\t Validation loss: 0.01333\t accuracy: 0.5716\n",
      "Epoch: 57\t Train loss: 0.01108\t accuracy: 0.5856\t Validation loss: 0.01302\t accuracy: 0.5642\n",
      "Epoch: 58\t Train loss: 0.01157\t accuracy: 0.5884\t Validation loss: 0.01347\t accuracy: 0.5705\n",
      "Epoch: 59\t Train loss: 0.01129\t accuracy: 0.5803\t Validation loss: 0.01334\t accuracy: 0.569\n",
      "Epoch: 60\t Train loss: 0.01119\t accuracy: 0.5927\t Validation loss: 0.01326\t accuracy: 0.5779\n",
      "Epoch: 61\t Train loss: 0.01114\t accuracy: 0.5927\t Validation loss: 0.01361\t accuracy: 0.5593\n",
      "Epoch: 62\t Train loss: 0.01094\t accuracy: 0.5938\t Validation loss: 0.0132\t accuracy: 0.5607\n",
      "Epoch: 63\t Train loss: 0.01118\t accuracy: 0.5866\t Validation loss: 0.01379\t accuracy: 0.5476\n",
      "Epoch: 64\t Train loss: 0.01094\t accuracy: 0.598\t Validation loss: 0.01323\t accuracy: 0.5549\n",
      "Epoch: 65\t Train loss: 0.01085\t accuracy: 0.6016\t Validation loss: 0.01342\t accuracy: 0.5632\n",
      "Epoch: 66\t Train loss: 0.01065\t accuracy: 0.603\t Validation loss: 0.01333\t accuracy: 0.5676\n",
      "Epoch: 67\t Train loss: 0.01092\t accuracy: 0.6023\t Validation loss: 0.01394\t accuracy: 0.5539\n",
      "Epoch: 68\t Train loss: 0.01075\t accuracy: 0.5913\t Validation loss: 0.01317\t accuracy: 0.57\n",
      "Epoch: 69\t Train loss: 0.01078\t accuracy: 0.6009\t Validation loss: 0.01401\t accuracy: 0.5711\n",
      "Epoch: 70\t Train loss: 0.01085\t accuracy: 0.6023\t Validation loss: 0.01303\t accuracy: 0.5622\n",
      "Epoch: 71\t Train loss: 0.01045\t accuracy: 0.6129\t Validation loss: 0.01323\t accuracy: 0.57\n",
      "Epoch: 72\t Train loss: 0.01062\t accuracy: 0.6115\t Validation loss: 0.01357\t accuracy: 0.5715\n",
      "Epoch: 73\t Train loss: 0.01043\t accuracy: 0.6026\t Validation loss: 0.01354\t accuracy: 0.5563\n",
      "Epoch: 74\t Train loss: 0.01037\t accuracy: 0.6044\t Validation loss: 0.01344\t accuracy: 0.5647\n",
      "Epoch: 75\t Train loss: 0.01052\t accuracy: 0.6051\t Validation loss: 0.01366\t accuracy: 0.552\n",
      "Epoch: 76\t Train loss: 0.01028\t accuracy: 0.608\t Validation loss: 0.01335\t accuracy: 0.5583\n",
      "Epoch: 77\t Train loss: 0.01033\t accuracy: 0.6115\t Validation loss: 0.01331\t accuracy: 0.5677\n",
      "Epoch: 78\t Train loss: 0.01033\t accuracy: 0.6108\t Validation loss: 0.01321\t accuracy: 0.5686\n",
      "Epoch: 79\t Train loss: 0.009964\t accuracy: 0.6158\t Validation loss: 0.0132\t accuracy: 0.5745\n",
      "Epoch: 80\t Train loss: 0.01003\t accuracy: 0.614\t Validation loss: 0.01355\t accuracy: 0.5583\n",
      "Epoch: 81\t Train loss: 0.01011\t accuracy: 0.6175\t Validation loss: 0.01353\t accuracy: 0.5647\n",
      "Epoch: 82\t Train loss: 0.01025\t accuracy: 0.6108\t Validation loss: 0.01361\t accuracy: 0.5682\n",
      "Epoch: 83\t Train loss: 0.009981\t accuracy: 0.6133\t Validation loss: 0.0135\t accuracy: 0.5642\n",
      "Epoch: 84\t Train loss: 0.01004\t accuracy: 0.6158\t Validation loss: 0.01393\t accuracy: 0.5632\n",
      "Epoch: 85\t Train loss: 0.01024\t accuracy: 0.6087\t Validation loss: 0.01338\t accuracy: 0.5618\n",
      "Epoch: 86\t Train loss: 0.009886\t accuracy: 0.6179\t Validation loss: 0.01376\t accuracy: 0.5819\n",
      "Epoch: 87\t Train loss: 0.01004\t accuracy: 0.6197\t Validation loss: 0.01374\t accuracy: 0.5696\n",
      "Epoch: 88\t Train loss: 0.009784\t accuracy: 0.62\t Validation loss: 0.0135\t accuracy: 0.5657\n",
      "Epoch: 89\t Train loss: 0.01004\t accuracy: 0.619\t Validation loss: 0.01352\t accuracy: 0.55\n",
      "Epoch: 90\t Train loss: 0.01046\t accuracy: 0.6058\t Validation loss: 0.01326\t accuracy: 0.5627\n",
      "Epoch: 91\t Train loss: 0.01002\t accuracy: 0.6101\t Validation loss: 0.01317\t accuracy: 0.5604\n",
      "Epoch: 92\t Train loss: 0.01009\t accuracy: 0.6133\t Validation loss: 0.01364\t accuracy: 0.5658\n",
      "Epoch: 93\t Train loss: 0.01008\t accuracy: 0.6257\t Validation loss: 0.01332\t accuracy: 0.5627\n",
      "Epoch: 94\t Train loss: 0.009729\t accuracy: 0.6222\t Validation loss: 0.01347\t accuracy: 0.572\n",
      "Epoch: 95\t Train loss: 0.009908\t accuracy: 0.6246\t Validation loss: 0.01364\t accuracy: 0.5667\n",
      "Epoch: 96\t Train loss: 0.009904\t accuracy: 0.6232\t Validation loss: 0.01335\t accuracy: 0.576\n",
      "Epoch: 97\t Train loss: 0.009757\t accuracy: 0.6222\t Validation loss: 0.01309\t accuracy: 0.5735\n",
      "Epoch: 98\t Train loss: 0.009684\t accuracy: 0.6236\t Validation loss: 0.01337\t accuracy: 0.5578\n",
      "Epoch: 99\t Train loss: 0.009373\t accuracy: 0.6403\t Validation loss: 0.01334\t accuracy: 0.5667\n",
      "Epoch: 100\t Train loss: 0.009466\t accuracy: 0.6204\t Validation loss: 0.0132\t accuracy: 0.5721\n",
      "Epoch: 101\t Train loss: 0.00926\t accuracy: 0.6349\t Validation loss: 0.0132\t accuracy: 0.5774\n",
      "Epoch: 102\t Train loss: 0.009401\t accuracy: 0.631\t Validation loss: 0.01327\t accuracy: 0.5696\n",
      "Epoch: 103\t Train loss: 0.009443\t accuracy: 0.6357\t Validation loss: 0.01335\t accuracy: 0.5706\n",
      "Epoch: 104\t Train loss: 0.009474\t accuracy: 0.6293\t Validation loss: 0.01351\t accuracy: 0.5593\n",
      "Epoch: 105\t Train loss: 0.009616\t accuracy: 0.6367\t Validation loss: 0.01345\t accuracy: 0.5604\n",
      "Epoch: 106\t Train loss: 0.009333\t accuracy: 0.6261\t Validation loss: 0.01335\t accuracy: 0.5677\n",
      "Epoch: 107\t Train loss: 0.009299\t accuracy: 0.6353\t Validation loss: 0.01336\t accuracy: 0.571\n",
      "Epoch: 108\t Train loss: 0.009127\t accuracy: 0.6403\t Validation loss: 0.01361\t accuracy: 0.5613\n",
      "Epoch: 109\t Train loss: 0.009123\t accuracy: 0.6403\t Validation loss: 0.01362\t accuracy: 0.5573\n",
      "Epoch: 110\t Train loss: 0.0094\t accuracy: 0.6296\t Validation loss: 0.01331\t accuracy: 0.5563\n",
      "Epoch: 111\t Train loss: 0.009235\t accuracy: 0.6207\t Validation loss: 0.01335\t accuracy: 0.5705\n",
      "Epoch: 112\t Train loss: 0.009314\t accuracy: 0.6239\t Validation loss: 0.01371\t accuracy: 0.5707\n",
      "Epoch: 113\t Train loss: 0.009288\t accuracy: 0.6435\t Validation loss: 0.01351\t accuracy: 0.5681\n",
      "Epoch: 114\t Train loss: 0.009012\t accuracy: 0.6413\t Validation loss: 0.01344\t accuracy: 0.5662\n",
      "Epoch: 115\t Train loss: 0.00945\t accuracy: 0.6381\t Validation loss: 0.0136\t accuracy: 0.5765\n",
      "Epoch: 116\t Train loss: 0.009295\t accuracy: 0.6392\t Validation loss: 0.01311\t accuracy: 0.5765\n",
      "Epoch: 117\t Train loss: 0.008977\t accuracy: 0.6534\t Validation loss: 0.01347\t accuracy: 0.5769\n",
      "Epoch: 118\t Train loss: 0.008853\t accuracy: 0.6374\t Validation loss: 0.01344\t accuracy: 0.5755\n",
      "Epoch: 119\t Train loss: 0.008767\t accuracy: 0.6445\t Validation loss: 0.01333\t accuracy: 0.5736\n",
      "Epoch: 120\t Train loss: 0.009319\t accuracy: 0.63\t Validation loss: 0.01356\t accuracy: 0.572\n",
      "Epoch: 121\t Train loss: 0.009188\t accuracy: 0.6357\t Validation loss: 0.01304\t accuracy: 0.5735\n",
      "Epoch: 122\t Train loss: 0.00895\t accuracy: 0.6406\t Validation loss: 0.01355\t accuracy: 0.5505\n",
      "Epoch: 123\t Train loss: 0.008852\t accuracy: 0.6452\t Validation loss: 0.01361\t accuracy: 0.556\n",
      "Epoch: 124\t Train loss: 0.009055\t accuracy: 0.6456\t Validation loss: 0.0135\t accuracy: 0.5687\n",
      "Epoch: 125\t Train loss: 0.00858\t accuracy: 0.6566\t Validation loss: 0.0131\t accuracy: 0.5658\n",
      "Epoch: 126\t Train loss: 0.008625\t accuracy: 0.6428\t Validation loss: 0.0136\t accuracy: 0.5711\n",
      "Epoch: 127\t Train loss: 0.008788\t accuracy: 0.6388\t Validation loss: 0.01316\t accuracy: 0.5764\n",
      "Epoch: 128\t Train loss: 0.008403\t accuracy: 0.6548\t Validation loss: 0.01375\t accuracy: 0.5519\n",
      "Epoch: 129\t Train loss: 0.009048\t accuracy: 0.6385\t Validation loss: 0.01306\t accuracy: 0.5907\n",
      "Epoch: 130\t Train loss: 0.008819\t accuracy: 0.6566\t Validation loss: 0.01334\t accuracy: 0.5819\n",
      "Epoch: 131\t Train loss: 0.008739\t accuracy: 0.6559\t Validation loss: 0.01354\t accuracy: 0.5663\n",
      "Epoch: 132\t Train loss: 0.008639\t accuracy: 0.652\t Validation loss: 0.01355\t accuracy: 0.5702\n",
      "Epoch: 133\t Train loss: 0.008483\t accuracy: 0.658\t Validation loss: 0.0133\t accuracy: 0.5785\n",
      "Epoch: 134\t Train loss: 0.008483\t accuracy: 0.6499\t Validation loss: 0.0132\t accuracy: 0.5745\n",
      "Epoch: 135\t Train loss: 0.008118\t accuracy: 0.6722\t Validation loss: 0.01337\t accuracy: 0.5745\n",
      "Epoch: 136\t Train loss: 0.008573\t accuracy: 0.657\t Validation loss: 0.01347\t accuracy: 0.578\n",
      "Epoch: 137\t Train loss: 0.008473\t accuracy: 0.6673\t Validation loss: 0.01332\t accuracy: 0.5784\n",
      "Epoch: 138\t Train loss: 0.008074\t accuracy: 0.6605\t Validation loss: 0.01368\t accuracy: 0.5647\n",
      "Epoch: 139\t Train loss: 0.008829\t accuracy: 0.6417\t Validation loss: 0.01351\t accuracy: 0.5809\n",
      "Epoch: 140\t Train loss: 0.008599\t accuracy: 0.6484\t Validation loss: 0.01305\t accuracy: 0.5795\n",
      "Epoch: 141\t Train loss: 0.00843\t accuracy: 0.6577\t Validation loss: 0.01328\t accuracy: 0.5764\n",
      "Epoch: 142\t Train loss: 0.008082\t accuracy: 0.6591\t Validation loss: 0.01373\t accuracy: 0.5643\n",
      "Epoch: 143\t Train loss: 0.008536\t accuracy: 0.6523\t Validation loss: 0.01337\t accuracy: 0.5804\n",
      "Epoch: 144\t Train loss: 0.008209\t accuracy: 0.6665\t Validation loss: 0.01351\t accuracy: 0.5735\n",
      "Epoch: 145\t Train loss: 0.008458\t accuracy: 0.6548\t Validation loss: 0.01367\t accuracy: 0.5834\n",
      "Epoch: 146\t Train loss: 0.008279\t accuracy: 0.6641\t Validation loss: 0.01356\t accuracy: 0.5745\n",
      "Epoch: 147\t Train loss: 0.007913\t accuracy: 0.6751\t Validation loss: 0.01355\t accuracy: 0.576\n",
      "Epoch: 148\t Train loss: 0.007948\t accuracy: 0.668\t Validation loss: 0.01354\t accuracy: 0.5779\n",
      "Epoch: 149\t Train loss: 0.007986\t accuracy: 0.6815\t Validation loss: 0.01357\t accuracy: 0.5668\n",
      "Epoch: 150\t Train loss: 0.008235\t accuracy: 0.669\t Validation loss: 0.01341\t accuracy: 0.5736\n",
      "Epoch: 151\t Train loss: 0.008578\t accuracy: 0.6673\t Validation loss: 0.01312\t accuracy: 0.5799\n",
      "Epoch: 152\t Train loss: 0.00828\t accuracy: 0.6662\t Validation loss: 0.0132\t accuracy: 0.576\n",
      "Epoch: 153\t Train loss: 0.008201\t accuracy: 0.6637\t Validation loss: 0.01341\t accuracy: 0.5779\n",
      "Epoch: 154\t Train loss: 0.007905\t accuracy: 0.6697\t Validation loss: 0.01338\t accuracy: 0.5779\n",
      "Epoch: 155\t Train loss: 0.008221\t accuracy: 0.6605\t Validation loss: 0.01337\t accuracy: 0.5599\n",
      "Epoch: 156\t Train loss: 0.008117\t accuracy: 0.6776\t Validation loss: 0.01316\t accuracy: 0.5697\n",
      "Epoch: 157\t Train loss: 0.00746\t accuracy: 0.6914\t Validation loss: 0.01315\t accuracy: 0.5707\n",
      "Epoch: 158\t Train loss: 0.007842\t accuracy: 0.68\t Validation loss: 0.01333\t accuracy: 0.5735\n",
      "Epoch: 159\t Train loss: 0.007818\t accuracy: 0.6747\t Validation loss: 0.01294\t accuracy: 0.5936\n",
      "Epoch: 160\t Train loss: 0.008064\t accuracy: 0.6729\t Validation loss: 0.01423\t accuracy: 0.5545\n",
      "Epoch: 161\t Train loss: 0.008109\t accuracy: 0.6719\t Validation loss: 0.01383\t accuracy: 0.5672\n",
      "Epoch: 162\t Train loss: 0.007824\t accuracy: 0.6776\t Validation loss: 0.01325\t accuracy: 0.5833\n",
      "Epoch: 163\t Train loss: 0.007976\t accuracy: 0.6665\t Validation loss: 0.01393\t accuracy: 0.5643\n",
      "Epoch: 164\t Train loss: 0.007845\t accuracy: 0.6687\t Validation loss: 0.01322\t accuracy: 0.5917\n",
      "Epoch: 165\t Train loss: 0.008123\t accuracy: 0.6744\t Validation loss: 0.01308\t accuracy: 0.5849\n",
      "Epoch: 166\t Train loss: 0.007717\t accuracy: 0.6783\t Validation loss: 0.01386\t accuracy: 0.5598\n",
      "Epoch: 167\t Train loss: 0.007782\t accuracy: 0.6836\t Validation loss: 0.01284\t accuracy: 0.5897\n",
      "Epoch: 168\t Train loss: 0.007416\t accuracy: 0.6839\t Validation loss: 0.01332\t accuracy: 0.5712\n",
      "Epoch: 169\t Train loss: 0.00767\t accuracy: 0.69\t Validation loss: 0.01344\t accuracy: 0.5745\n",
      "Epoch: 170\t Train loss: 0.008084\t accuracy: 0.6797\t Validation loss: 0.01298\t accuracy: 0.5614\n",
      "Epoch: 171\t Train loss: 0.007662\t accuracy: 0.6779\t Validation loss: 0.01358\t accuracy: 0.5779\n",
      "Epoch: 172\t Train loss: 0.007419\t accuracy: 0.6953\t Validation loss: 0.01279\t accuracy: 0.5961\n",
      "Epoch: 173\t Train loss: 0.007734\t accuracy: 0.6797\t Validation loss: 0.01348\t accuracy: 0.5672\n",
      "Epoch: 174\t Train loss: 0.007481\t accuracy: 0.68\t Validation loss: 0.01333\t accuracy: 0.5706\n",
      "Epoch: 175\t Train loss: 0.007737\t accuracy: 0.6772\t Validation loss: 0.01286\t accuracy: 0.5868\n",
      "Epoch: 176\t Train loss: 0.007407\t accuracy: 0.6879\t Validation loss: 0.01321\t accuracy: 0.5853\n",
      "Epoch: 177\t Train loss: 0.007582\t accuracy: 0.6829\t Validation loss: 0.01333\t accuracy: 0.573\n",
      "Epoch: 178\t Train loss: 0.007927\t accuracy: 0.6786\t Validation loss: 0.01295\t accuracy: 0.6\n",
      "Epoch: 179\t Train loss: 0.007491\t accuracy: 0.6836\t Validation loss: 0.01305\t accuracy: 0.5779\n",
      "Epoch: 180\t Train loss: 0.007486\t accuracy: 0.6903\t Validation loss: 0.01335\t accuracy: 0.5647\n",
      "Epoch: 181\t Train loss: 0.007321\t accuracy: 0.6911\t Validation loss: 0.01316\t accuracy: 0.5891\n",
      "Epoch: 182\t Train loss: 0.007585\t accuracy: 0.68\t Validation loss: 0.01343\t accuracy: 0.5598\n",
      "Epoch: 183\t Train loss: 0.007443\t accuracy: 0.6811\t Validation loss: 0.01282\t accuracy: 0.5833\n",
      "Epoch: 184\t Train loss: 0.007317\t accuracy: 0.7045\t Validation loss: 0.01356\t accuracy: 0.5731\n",
      "Epoch: 185\t Train loss: 0.0073\t accuracy: 0.7006\t Validation loss: 0.01316\t accuracy: 0.5731\n",
      "Epoch: 186\t Train loss: 0.007229\t accuracy: 0.6978\t Validation loss: 0.01336\t accuracy: 0.5736\n",
      "Epoch: 187\t Train loss: 0.007213\t accuracy: 0.6996\t Validation loss: 0.01349\t accuracy: 0.5755\n",
      "Epoch: 188\t Train loss: 0.007196\t accuracy: 0.6907\t Validation loss: 0.01308\t accuracy: 0.5741\n",
      "Epoch: 189\t Train loss: 0.007254\t accuracy: 0.6964\t Validation loss: 0.0133\t accuracy: 0.5667\n",
      "Epoch: 190\t Train loss: 0.007518\t accuracy: 0.6932\t Validation loss: 0.01358\t accuracy: 0.5661\n",
      "Epoch: 191\t Train loss: 0.007472\t accuracy: 0.6889\t Validation loss: 0.01291\t accuracy: 0.5955\n",
      "Epoch: 192\t Train loss: 0.007072\t accuracy: 0.7056\t Validation loss: 0.01317\t accuracy: 0.5833\n",
      "Epoch: 193\t Train loss: 0.00726\t accuracy: 0.7013\t Validation loss: 0.01324\t accuracy: 0.5749\n",
      "Epoch: 194\t Train loss: 0.007088\t accuracy: 0.7049\t Validation loss: 0.01278\t accuracy: 0.5813\n",
      "Epoch: 195\t Train loss: 0.007007\t accuracy: 0.7056\t Validation loss: 0.01336\t accuracy: 0.5794\n",
      "Epoch: 196\t Train loss: 0.007525\t accuracy: 0.6825\t Validation loss: 0.01333\t accuracy: 0.5812\n",
      "Epoch: 197\t Train loss: 0.007424\t accuracy: 0.6889\t Validation loss: 0.01288\t accuracy: 0.5828\n",
      "Epoch: 198\t Train loss: 0.007232\t accuracy: 0.6935\t Validation loss: 0.0129\t accuracy: 0.5843\n",
      "Epoch: 199\t Train loss: 0.007048\t accuracy: 0.707\t Validation loss: 0.01334\t accuracy: 0.5739\n",
      "Epoch: 200\t Train loss: 0.007108\t accuracy: 0.707\t Validation loss: 0.0129\t accuracy: 0.5931\n",
      "Epoch: 201\t Train loss: 0.007332\t accuracy: 0.7003\t Validation loss: 0.013\t accuracy: 0.5769\n",
      "Epoch: 202\t Train loss: 0.007226\t accuracy: 0.701\t Validation loss: 0.0128\t accuracy: 0.5946\n",
      "Epoch: 203\t Train loss: 0.007369\t accuracy: 0.6982\t Validation loss: 0.01261\t accuracy: 0.5906\n",
      "Epoch: 204\t Train loss: 0.007105\t accuracy: 0.6957\t Validation loss: 0.01312\t accuracy: 0.5814\n",
      "Epoch: 205\t Train loss: 0.006924\t accuracy: 0.7053\t Validation loss: 0.01309\t accuracy: 0.5725\n",
      "Epoch: 206\t Train loss: 0.006948\t accuracy: 0.7031\t Validation loss: 0.01309\t accuracy: 0.5682\n",
      "Epoch: 207\t Train loss: 0.006553\t accuracy: 0.7116\t Validation loss: 0.01346\t accuracy: 0.5548\n",
      "Epoch: 208\t Train loss: 0.007328\t accuracy: 0.6989\t Validation loss: 0.0134\t accuracy: 0.5691\n",
      "Epoch: 209\t Train loss: 0.006829\t accuracy: 0.7031\t Validation loss: 0.01296\t accuracy: 0.5667\n",
      "Epoch: 210\t Train loss: 0.007223\t accuracy: 0.6946\t Validation loss: 0.01318\t accuracy: 0.5778\n",
      "Epoch: 211\t Train loss: 0.006921\t accuracy: 0.7092\t Validation loss: 0.01312\t accuracy: 0.5716\n",
      "Epoch: 212\t Train loss: 0.007046\t accuracy: 0.6996\t Validation loss: 0.0134\t accuracy: 0.5813\n",
      "Epoch: 213\t Train loss: 0.006706\t accuracy: 0.7138\t Validation loss: 0.01347\t accuracy: 0.5794\n",
      "Epoch: 214\t Train loss: 0.006758\t accuracy: 0.7099\t Validation loss: 0.01291\t accuracy: 0.5852\n",
      "Epoch: 215\t Train loss: 0.00679\t accuracy: 0.7102\t Validation loss: 0.01311\t accuracy: 0.5843\n",
      "Epoch: 216\t Train loss: 0.006799\t accuracy: 0.7106\t Validation loss: 0.01316\t accuracy: 0.5824\n",
      "Epoch: 217\t Train loss: 0.006946\t accuracy: 0.7028\t Validation loss: 0.01313\t accuracy: 0.5808\n",
      "Epoch: 218\t Train loss: 0.006821\t accuracy: 0.7159\t Validation loss: 0.0132\t accuracy: 0.5892\n",
      "Epoch: 219\t Train loss: 0.006931\t accuracy: 0.718\t Validation loss: 0.01314\t accuracy: 0.5745\n",
      "Epoch: 220\t Train loss: 0.007058\t accuracy: 0.7134\t Validation loss: 0.0132\t accuracy: 0.5775\n",
      "Epoch: 221\t Train loss: 0.006579\t accuracy: 0.7202\t Validation loss: 0.01306\t accuracy: 0.5897\n",
      "Epoch: 222\t Train loss: 0.006715\t accuracy: 0.7131\t Validation loss: 0.01367\t accuracy: 0.5652\n",
      "Epoch: 223\t Train loss: 0.006888\t accuracy: 0.7134\t Validation loss: 0.01324\t accuracy: 0.5887\n",
      "Epoch: 224\t Train loss: 0.006797\t accuracy: 0.6957\t Validation loss: 0.01296\t accuracy: 0.5765\n",
      "Epoch: 225\t Train loss: 0.006874\t accuracy: 0.7074\t Validation loss: 0.0133\t accuracy: 0.5696\n",
      "Epoch: 226\t Train loss: 0.006943\t accuracy: 0.6985\t Validation loss: 0.01308\t accuracy: 0.5946\n",
      "Epoch: 227\t Train loss: 0.006682\t accuracy: 0.7148\t Validation loss: 0.01322\t accuracy: 0.5813\n",
      "Epoch: 228\t Train loss: 0.006678\t accuracy: 0.7138\t Validation loss: 0.01362\t accuracy: 0.5735\n",
      "Epoch: 229\t Train loss: 0.006605\t accuracy: 0.7077\t Validation loss: 0.01302\t accuracy: 0.5823\n",
      "Epoch: 230\t Train loss: 0.006619\t accuracy: 0.7202\t Validation loss: 0.01306\t accuracy: 0.5828\n",
      "Epoch: 231\t Train loss: 0.006588\t accuracy: 0.7212\t Validation loss: 0.01314\t accuracy: 0.5857\n",
      "Epoch: 232\t Train loss: 0.006815\t accuracy: 0.7141\t Validation loss: 0.01346\t accuracy: 0.5731\n",
      "Epoch: 233\t Train loss: 0.006706\t accuracy: 0.7234\t Validation loss: 0.01299\t accuracy: 0.5833\n",
      "Epoch: 234\t Train loss: 0.006727\t accuracy: 0.718\t Validation loss: 0.01327\t accuracy: 0.5848\n",
      "Epoch: 235\t Train loss: 0.0068\t accuracy: 0.7212\t Validation loss: 0.01308\t accuracy: 0.5887\n",
      "Epoch: 236\t Train loss: 0.006381\t accuracy: 0.7234\t Validation loss: 0.01356\t accuracy: 0.5722\n",
      "Epoch: 237\t Train loss: 0.006691\t accuracy: 0.7173\t Validation loss: 0.01365\t accuracy: 0.5716\n",
      "Epoch: 238\t Train loss: 0.006678\t accuracy: 0.7184\t Validation loss: 0.01337\t accuracy: 0.5717\n",
      "Epoch: 239\t Train loss: 0.006583\t accuracy: 0.7152\t Validation loss: 0.01331\t accuracy: 0.5838\n",
      "Epoch: 240\t Train loss: 0.006312\t accuracy: 0.733\t Validation loss: 0.0135\t accuracy: 0.5984\n",
      "Epoch: 241\t Train loss: 0.006343\t accuracy: 0.7248\t Validation loss: 0.01373\t accuracy: 0.575\n",
      "Epoch: 242\t Train loss: 0.006583\t accuracy: 0.7156\t Validation loss: 0.01308\t accuracy: 0.5858\n",
      "Epoch: 243\t Train loss: 0.006473\t accuracy: 0.717\t Validation loss: 0.01324\t accuracy: 0.5598\n",
      "Epoch: 244\t Train loss: 0.006623\t accuracy: 0.7156\t Validation loss: 0.01317\t accuracy: 0.5623\n",
      "Epoch: 245\t Train loss: 0.006464\t accuracy: 0.7209\t Validation loss: 0.01318\t accuracy: 0.5672\n",
      "Epoch: 246\t Train loss: 0.006383\t accuracy: 0.7244\t Validation loss: 0.01297\t accuracy: 0.5774\n",
      "Epoch: 247\t Train loss: 0.006488\t accuracy: 0.7269\t Validation loss: 0.013\t accuracy: 0.5808\n",
      "Epoch: 248\t Train loss: 0.006282\t accuracy: 0.7319\t Validation loss: 0.01361\t accuracy: 0.5809\n",
      "Epoch: 249\t Train loss: 0.006633\t accuracy: 0.7205\t Validation loss: 0.01296\t accuracy: 0.5818\n",
      "Epoch: 250\t Train loss: 0.006222\t accuracy: 0.7386\t Validation loss: 0.01305\t accuracy: 0.5912\n",
      "Epoch: 251\t Train loss: 0.006461\t accuracy: 0.7223\t Validation loss: 0.01305\t accuracy: 0.5887\n",
      "Epoch: 252\t Train loss: 0.006655\t accuracy: 0.7219\t Validation loss: 0.01348\t accuracy: 0.579\n",
      "Epoch: 253\t Train loss: 0.006405\t accuracy: 0.7212\t Validation loss: 0.01295\t accuracy: 0.5951\n",
      "Epoch: 254\t Train loss: 0.006487\t accuracy: 0.7212\t Validation loss: 0.0133\t accuracy: 0.574\n",
      "Epoch: 255\t Train loss: 0.006594\t accuracy: 0.717\t Validation loss: 0.01279\t accuracy: 0.5853\n",
      "Epoch: 256\t Train loss: 0.006466\t accuracy: 0.7195\t Validation loss: 0.01356\t accuracy: 0.5627\n",
      "Epoch: 257\t Train loss: 0.006101\t accuracy: 0.7315\t Validation loss: 0.01299\t accuracy: 0.5838\n",
      "Epoch: 258\t Train loss: 0.006267\t accuracy: 0.7354\t Validation loss: 0.01314\t accuracy: 0.573\n",
      "Epoch: 259\t Train loss: 0.006249\t accuracy: 0.7326\t Validation loss: 0.0133\t accuracy: 0.5721\n",
      "Epoch: 260\t Train loss: 0.006433\t accuracy: 0.7251\t Validation loss: 0.0134\t accuracy: 0.5798\n",
      "Epoch: 261\t Train loss: 0.005994\t accuracy: 0.7429\t Validation loss: 0.01307\t accuracy: 0.5804\n",
      "Epoch: 262\t Train loss: 0.00617\t accuracy: 0.7362\t Validation loss: 0.01303\t accuracy: 0.5902\n",
      "Epoch: 263\t Train loss: 0.006309\t accuracy: 0.7191\t Validation loss: 0.01295\t accuracy: 0.5921\n",
      "Epoch: 264\t Train loss: 0.006107\t accuracy: 0.7344\t Validation loss: 0.0133\t accuracy: 0.5859\n",
      "Epoch: 265\t Train loss: 0.006501\t accuracy: 0.7212\t Validation loss: 0.0131\t accuracy: 0.5711\n",
      "Epoch: 266\t Train loss: 0.00633\t accuracy: 0.7262\t Validation loss: 0.01313\t accuracy: 0.5765\n",
      "Epoch: 267\t Train loss: 0.006262\t accuracy: 0.7337\t Validation loss: 0.01304\t accuracy: 0.5882\n",
      "Epoch: 268\t Train loss: 0.006166\t accuracy: 0.7433\t Validation loss: 0.01297\t accuracy: 0.5907\n",
      "Epoch: 269\t Train loss: 0.006206\t accuracy: 0.7262\t Validation loss: 0.01309\t accuracy: 0.5893\n",
      "Epoch: 270\t Train loss: 0.006299\t accuracy: 0.7319\t Validation loss: 0.01305\t accuracy: 0.5716\n",
      "Epoch: 271\t Train loss: 0.006409\t accuracy: 0.7219\t Validation loss: 0.0133\t accuracy: 0.5666\n",
      "Epoch: 272\t Train loss: 0.006085\t accuracy: 0.7347\t Validation loss: 0.01313\t accuracy: 0.5813\n",
      "Epoch: 273\t Train loss: 0.00604\t accuracy: 0.7433\t Validation loss: 0.01365\t accuracy: 0.5706\n",
      "Epoch: 274\t Train loss: 0.006275\t accuracy: 0.7273\t Validation loss: 0.01302\t accuracy: 0.5819\n",
      "Epoch: 275\t Train loss: 0.006057\t accuracy: 0.7369\t Validation loss: 0.01316\t accuracy: 0.5867\n",
      "Epoch: 276\t Train loss: 0.006107\t accuracy: 0.7422\t Validation loss: 0.01276\t accuracy: 0.5911\n",
      "Epoch: 277\t Train loss: 0.005967\t accuracy: 0.7372\t Validation loss: 0.01366\t accuracy: 0.5705\n",
      "Epoch: 278\t Train loss: 0.006497\t accuracy: 0.7276\t Validation loss: 0.01256\t accuracy: 0.5976\n",
      "Epoch: 279\t Train loss: 0.006258\t accuracy: 0.7276\t Validation loss: 0.01346\t accuracy: 0.5808\n",
      "Epoch: 280\t Train loss: 0.006439\t accuracy: 0.7269\t Validation loss: 0.01285\t accuracy: 0.5828\n",
      "Epoch: 281\t Train loss: 0.006229\t accuracy: 0.7298\t Validation loss: 0.013\t accuracy: 0.577\n",
      "Epoch: 282\t Train loss: 0.006103\t accuracy: 0.7372\t Validation loss: 0.01299\t accuracy: 0.5848\n",
      "Epoch: 283\t Train loss: 0.005991\t accuracy: 0.7312\t Validation loss: 0.01273\t accuracy: 0.5872\n",
      "Epoch: 284\t Train loss: 0.006052\t accuracy: 0.7489\t Validation loss: 0.01298\t accuracy: 0.5862\n",
      "Epoch: 285\t Train loss: 0.006057\t accuracy: 0.7443\t Validation loss: 0.01275\t accuracy: 0.5916\n",
      "Epoch: 286\t Train loss: 0.006126\t accuracy: 0.7401\t Validation loss: 0.01288\t accuracy: 0.576\n",
      "Epoch: 287\t Train loss: 0.005887\t accuracy: 0.7422\t Validation loss: 0.01281\t accuracy: 0.5824\n",
      "Epoch: 288\t Train loss: 0.005819\t accuracy: 0.7468\t Validation loss: 0.01283\t accuracy: 0.5916\n",
      "Epoch: 289\t Train loss: 0.005858\t accuracy: 0.7443\t Validation loss: 0.01292\t accuracy: 0.5878\n",
      "Epoch: 290\t Train loss: 0.006004\t accuracy: 0.7493\t Validation loss: 0.01291\t accuracy: 0.5911\n",
      "Epoch: 291\t Train loss: 0.005948\t accuracy: 0.7489\t Validation loss: 0.01282\t accuracy: 0.5847\n",
      "Epoch: 292\t Train loss: 0.006103\t accuracy: 0.7415\t Validation loss: 0.01297\t accuracy: 0.5706\n",
      "Epoch: 293\t Train loss: 0.005896\t accuracy: 0.7386\t Validation loss: 0.0128\t accuracy: 0.5902\n",
      "Epoch: 294\t Train loss: 0.005915\t accuracy: 0.7457\t Validation loss: 0.01304\t accuracy: 0.5848\n",
      "Epoch: 295\t Train loss: 0.006005\t accuracy: 0.7379\t Validation loss: 0.01277\t accuracy: 0.5902\n",
      "Epoch: 296\t Train loss: 0.00591\t accuracy: 0.745\t Validation loss: 0.01326\t accuracy: 0.5853\n",
      "Epoch: 297\t Train loss: 0.005867\t accuracy: 0.7415\t Validation loss: 0.01294\t accuracy: 0.5911\n",
      "Epoch: 298\t Train loss: 0.005818\t accuracy: 0.745\t Validation loss: 0.01266\t accuracy: 0.5976\n",
      "Epoch: 299\t Train loss: 0.005753\t accuracy: 0.7401\t Validation loss: 0.0132\t accuracy: 0.5765\n",
      "Epoch: 300\t Train loss: 0.005777\t accuracy: 0.7443\t Validation loss: 0.01304\t accuracy: 0.597\n",
      "Epoch: 301\t Train loss: 0.006027\t accuracy: 0.7298\t Validation loss: 0.01312\t accuracy: 0.5951\n",
      "Epoch: 302\t Train loss: 0.006102\t accuracy: 0.739\t Validation loss: 0.01275\t accuracy: 0.5871\n",
      "Epoch: 303\t Train loss: 0.005649\t accuracy: 0.7489\t Validation loss: 0.01309\t accuracy: 0.5716\n",
      "Epoch: 304\t Train loss: 0.005432\t accuracy: 0.7564\t Validation loss: 0.01321\t accuracy: 0.5858\n",
      "Epoch: 305\t Train loss: 0.005717\t accuracy: 0.7496\t Validation loss: 0.01294\t accuracy: 0.5838\n",
      "Epoch: 306\t Train loss: 0.005499\t accuracy: 0.7585\t Validation loss: 0.01271\t accuracy: 0.5906\n",
      "Epoch: 307\t Train loss: 0.005779\t accuracy: 0.7418\t Validation loss: 0.01269\t accuracy: 0.5916\n",
      "Epoch: 308\t Train loss: 0.00558\t accuracy: 0.7507\t Validation loss: 0.01315\t accuracy: 0.5642\n",
      "Epoch: 309\t Train loss: 0.006\t accuracy: 0.7422\t Validation loss: 0.01279\t accuracy: 0.5814\n",
      "Epoch: 310\t Train loss: 0.005726\t accuracy: 0.7468\t Validation loss: 0.01297\t accuracy: 0.5897\n",
      "Epoch: 311\t Train loss: 0.006126\t accuracy: 0.7305\t Validation loss: 0.01305\t accuracy: 0.5897\n",
      "Epoch: 312\t Train loss: 0.005528\t accuracy: 0.7575\t Validation loss: 0.01297\t accuracy: 0.5779\n",
      "Epoch: 313\t Train loss: 0.005758\t accuracy: 0.7401\t Validation loss: 0.01262\t accuracy: 0.5878\n",
      "Epoch: 314\t Train loss: 0.005805\t accuracy: 0.7372\t Validation loss: 0.01256\t accuracy: 0.5931\n",
      "Epoch: 315\t Train loss: 0.0057\t accuracy: 0.7571\t Validation loss: 0.01336\t accuracy: 0.5779\n",
      "Epoch: 316\t Train loss: 0.005624\t accuracy: 0.7468\t Validation loss: 0.01303\t accuracy: 0.5878\n",
      "Epoch: 317\t Train loss: 0.005733\t accuracy: 0.7358\t Validation loss: 0.01305\t accuracy: 0.5701\n",
      "Epoch: 318\t Train loss: 0.005687\t accuracy: 0.7575\t Validation loss: 0.0131\t accuracy: 0.5804\n",
      "Epoch: 319\t Train loss: 0.00585\t accuracy: 0.7479\t Validation loss: 0.01327\t accuracy: 0.5769\n",
      "Epoch: 320\t Train loss: 0.005601\t accuracy: 0.7496\t Validation loss: 0.01274\t accuracy: 0.5847\n",
      "Epoch: 321\t Train loss: 0.005573\t accuracy: 0.7607\t Validation loss: 0.01288\t accuracy: 0.5975\n",
      "Epoch: 322\t Train loss: 0.005866\t accuracy: 0.7468\t Validation loss: 0.01296\t accuracy: 0.5872\n",
      "Epoch: 323\t Train loss: 0.005714\t accuracy: 0.7578\t Validation loss: 0.01301\t accuracy: 0.5922\n",
      "Epoch: 324\t Train loss: 0.00557\t accuracy: 0.7553\t Validation loss: 0.01307\t accuracy: 0.5917\n",
      "Epoch: 325\t Train loss: 0.005548\t accuracy: 0.7468\t Validation loss: 0.01273\t accuracy: 0.5984\n",
      "Epoch: 326\t Train loss: 0.005919\t accuracy: 0.7511\t Validation loss: 0.01264\t accuracy: 0.5878\n",
      "Epoch: 327\t Train loss: 0.005546\t accuracy: 0.7521\t Validation loss: 0.01283\t accuracy: 0.5819\n",
      "Epoch: 328\t Train loss: 0.005543\t accuracy: 0.7628\t Validation loss: 0.01312\t accuracy: 0.5883\n",
      "Epoch: 329\t Train loss: 0.005472\t accuracy: 0.7589\t Validation loss: 0.01293\t accuracy: 0.5892\n",
      "Epoch: 330\t Train loss: 0.005493\t accuracy: 0.7624\t Validation loss: 0.01274\t accuracy: 0.5868\n",
      "Epoch: 331\t Train loss: 0.005532\t accuracy: 0.7415\t Validation loss: 0.01315\t accuracy: 0.5751\n",
      "Epoch: 332\t Train loss: 0.005589\t accuracy: 0.7536\t Validation loss: 0.01262\t accuracy: 0.5868\n",
      "Epoch: 333\t Train loss: 0.005414\t accuracy: 0.7596\t Validation loss: 0.013\t accuracy: 0.5877\n",
      "Epoch: 334\t Train loss: 0.005312\t accuracy: 0.767\t Validation loss: 0.01303\t accuracy: 0.577\n",
      "Epoch: 335\t Train loss: 0.005818\t accuracy: 0.7504\t Validation loss: 0.01263\t accuracy: 0.6009\n",
      "Epoch: 336\t Train loss: 0.005364\t accuracy: 0.7536\t Validation loss: 0.01261\t accuracy: 0.5829\n",
      "Epoch: 337\t Train loss: 0.005645\t accuracy: 0.7518\t Validation loss: 0.01293\t accuracy: 0.5858\n",
      "Epoch: 338\t Train loss: 0.005603\t accuracy: 0.7557\t Validation loss: 0.01299\t accuracy: 0.5843\n",
      "Epoch: 339\t Train loss: 0.005643\t accuracy: 0.75\t Validation loss: 0.0127\t accuracy: 0.5926\n",
      "Epoch: 340\t Train loss: 0.005462\t accuracy: 0.7667\t Validation loss: 0.0129\t accuracy: 0.5883\n",
      "Epoch: 341\t Train loss: 0.005755\t accuracy: 0.7546\t Validation loss: 0.01268\t accuracy: 0.5995\n",
      "Epoch: 342\t Train loss: 0.005118\t accuracy: 0.7727\t Validation loss: 0.01289\t accuracy: 0.5882\n",
      "Epoch: 343\t Train loss: 0.005494\t accuracy: 0.7592\t Validation loss: 0.0127\t accuracy: 0.5966\n",
      "Epoch: 344\t Train loss: 0.005295\t accuracy: 0.7656\t Validation loss: 0.01301\t accuracy: 0.598\n",
      "Epoch: 345\t Train loss: 0.005397\t accuracy: 0.7489\t Validation loss: 0.01293\t accuracy: 0.5941\n",
      "Epoch: 346\t Train loss: 0.005417\t accuracy: 0.7546\t Validation loss: 0.01271\t accuracy: 0.5887\n",
      "Epoch: 347\t Train loss: 0.005377\t accuracy: 0.7585\t Validation loss: 0.01308\t accuracy: 0.5883\n",
      "Epoch: 348\t Train loss: 0.005427\t accuracy: 0.7635\t Validation loss: 0.01305\t accuracy: 0.5995\n",
      "Epoch: 349\t Train loss: 0.005668\t accuracy: 0.755\t Validation loss: 0.01298\t accuracy: 0.58\n",
      "Epoch: 350\t Train loss: 0.00542\t accuracy: 0.7614\t Validation loss: 0.01282\t accuracy: 0.5985\n",
      "Epoch: 351\t Train loss: 0.005222\t accuracy: 0.7731\t Validation loss: 0.01296\t accuracy: 0.6048\n",
      "Epoch: 352\t Train loss: 0.005505\t accuracy: 0.7617\t Validation loss: 0.01289\t accuracy: 0.5984\n",
      "Epoch: 353\t Train loss: 0.005095\t accuracy: 0.772\t Validation loss: 0.01276\t accuracy: 0.5843\n",
      "Epoch: 354\t Train loss: 0.005343\t accuracy: 0.771\t Validation loss: 0.01303\t accuracy: 0.5926\n",
      "Epoch: 355\t Train loss: 0.005514\t accuracy: 0.777\t Validation loss: 0.01263\t accuracy: 0.5994\n",
      "Epoch: 356\t Train loss: 0.005481\t accuracy: 0.7681\t Validation loss: 0.01282\t accuracy: 0.5916\n",
      "Epoch: 357\t Train loss: 0.005455\t accuracy: 0.7621\t Validation loss: 0.01305\t accuracy: 0.5905\n",
      "Epoch: 358\t Train loss: 0.005324\t accuracy: 0.7518\t Validation loss: 0.01316\t accuracy: 0.5936\n",
      "Epoch: 359\t Train loss: 0.005495\t accuracy: 0.7717\t Validation loss: 0.01304\t accuracy: 0.5638\n",
      "Epoch: 360\t Train loss: 0.005527\t accuracy: 0.7638\t Validation loss: 0.01283\t accuracy: 0.5882\n",
      "Epoch: 361\t Train loss: 0.005356\t accuracy: 0.761\t Validation loss: 0.01263\t accuracy: 0.5886\n",
      "Epoch: 362\t Train loss: 0.005316\t accuracy: 0.7621\t Validation loss: 0.01306\t accuracy: 0.5784\n",
      "Epoch: 363\t Train loss: 0.005386\t accuracy: 0.7628\t Validation loss: 0.01277\t accuracy: 0.5902\n",
      "Epoch: 364\t Train loss: 0.005424\t accuracy: 0.7674\t Validation loss: 0.01277\t accuracy: 0.5945\n",
      "Epoch: 365\t Train loss: 0.005549\t accuracy: 0.761\t Validation loss: 0.01328\t accuracy: 0.5755\n",
      "Epoch: 366\t Train loss: 0.005666\t accuracy: 0.7472\t Validation loss: 0.01289\t accuracy: 0.5955\n",
      "Epoch: 367\t Train loss: 0.005409\t accuracy: 0.7546\t Validation loss: 0.01262\t accuracy: 0.6068\n",
      "Epoch: 368\t Train loss: 0.005568\t accuracy: 0.7646\t Validation loss: 0.01297\t accuracy: 0.5794\n",
      "Epoch: 369\t Train loss: 0.005523\t accuracy: 0.7674\t Validation loss: 0.0127\t accuracy: 0.6009\n",
      "Epoch: 370\t Train loss: 0.005468\t accuracy: 0.7681\t Validation loss: 0.01279\t accuracy: 0.5881\n",
      "Epoch: 371\t Train loss: 0.005403\t accuracy: 0.7667\t Validation loss: 0.01272\t accuracy: 0.5998\n",
      "Epoch: 372\t Train loss: 0.005069\t accuracy: 0.7837\t Validation loss: 0.01298\t accuracy: 0.5808\n",
      "Epoch: 373\t Train loss: 0.005284\t accuracy: 0.771\t Validation loss: 0.01321\t accuracy: 0.5877\n",
      "Epoch: 374\t Train loss: 0.005156\t accuracy: 0.7713\t Validation loss: 0.01324\t accuracy: 0.5779\n",
      "Epoch: 375\t Train loss: 0.005226\t accuracy: 0.7756\t Validation loss: 0.013\t accuracy: 0.5901\n",
      "Epoch: 376\t Train loss: 0.005262\t accuracy: 0.7706\t Validation loss: 0.01284\t accuracy: 0.5872\n",
      "Epoch: 377\t Train loss: 0.005446\t accuracy: 0.7731\t Validation loss: 0.01281\t accuracy: 0.5804\n",
      "Epoch: 378\t Train loss: 0.005203\t accuracy: 0.7646\t Validation loss: 0.01299\t accuracy: 0.578\n",
      "Epoch: 379\t Train loss: 0.005189\t accuracy: 0.7756\t Validation loss: 0.01318\t accuracy: 0.5808\n",
      "Epoch: 380\t Train loss: 0.005257\t accuracy: 0.7621\t Validation loss: 0.01316\t accuracy: 0.5779\n",
      "Epoch: 381\t Train loss: 0.004921\t accuracy: 0.783\t Validation loss: 0.01293\t accuracy: 0.594\n",
      "Epoch: 382\t Train loss: 0.005147\t accuracy: 0.7702\t Validation loss: 0.01301\t accuracy: 0.5906\n",
      "Epoch: 383\t Train loss: 0.005193\t accuracy: 0.7766\t Validation loss: 0.01291\t accuracy: 0.5965\n",
      "Epoch: 384\t Train loss: 0.005091\t accuracy: 0.7741\t Validation loss: 0.01266\t accuracy: 0.6068\n",
      "Epoch: 385\t Train loss: 0.005108\t accuracy: 0.7734\t Validation loss: 0.01289\t accuracy: 0.5941\n",
      "Epoch: 386\t Train loss: 0.005208\t accuracy: 0.7646\t Validation loss: 0.01346\t accuracy: 0.5711\n",
      "Epoch: 387\t Train loss: 0.005039\t accuracy: 0.7798\t Validation loss: 0.0128\t accuracy: 0.5985\n",
      "Epoch: 388\t Train loss: 0.005161\t accuracy: 0.7724\t Validation loss: 0.01282\t accuracy: 0.6004\n",
      "Epoch: 389\t Train loss: 0.00504\t accuracy: 0.7706\t Validation loss: 0.01287\t accuracy: 0.5872\n",
      "Epoch: 390\t Train loss: 0.005108\t accuracy: 0.7741\t Validation loss: 0.01294\t accuracy: 0.594\n",
      "Epoch: 391\t Train loss: 0.005191\t accuracy: 0.7681\t Validation loss: 0.01276\t accuracy: 0.6004\n",
      "Epoch: 392\t Train loss: 0.005039\t accuracy: 0.7816\t Validation loss: 0.01322\t accuracy: 0.5916\n",
      "Epoch: 393\t Train loss: 0.005276\t accuracy: 0.7631\t Validation loss: 0.01277\t accuracy: 0.597\n",
      "Epoch: 394\t Train loss: 0.005383\t accuracy: 0.7638\t Validation loss: 0.01298\t accuracy: 0.5916\n",
      "Epoch: 395\t Train loss: 0.005328\t accuracy: 0.7702\t Validation loss: 0.01277\t accuracy: 0.6097\n",
      "Epoch: 396\t Train loss: 0.005213\t accuracy: 0.7713\t Validation loss: 0.01318\t accuracy: 0.5867\n",
      "Epoch: 397\t Train loss: 0.00507\t accuracy: 0.7812\t Validation loss: 0.01254\t accuracy: 0.5984\n",
      "Epoch: 398\t Train loss: 0.005109\t accuracy: 0.7734\t Validation loss: 0.01316\t accuracy: 0.5857\n",
      "Epoch: 399\t Train loss: 0.005191\t accuracy: 0.7773\t Validation loss: 0.01274\t accuracy: 0.5877\n",
      "Epoch: 400\t Train loss: 0.005036\t accuracy: 0.7788\t Validation loss: 0.01316\t accuracy: 0.5853\n",
      "Epoch: 401\t Train loss: 0.004921\t accuracy: 0.7837\t Validation loss: 0.01293\t accuracy: 0.5906\n",
      "Epoch: 402\t Train loss: 0.005078\t accuracy: 0.7866\t Validation loss: 0.01249\t accuracy: 0.5906\n",
      "Epoch: 403\t Train loss: 0.005167\t accuracy: 0.7695\t Validation loss: 0.01284\t accuracy: 0.594\n",
      "Epoch: 404\t Train loss: 0.00514\t accuracy: 0.7763\t Validation loss: 0.01313\t accuracy: 0.5951\n",
      "Epoch: 405\t Train loss: 0.004977\t accuracy: 0.7749\t Validation loss: 0.01272\t accuracy: 0.5897\n",
      "Epoch: 406\t Train loss: 0.005251\t accuracy: 0.7777\t Validation loss: 0.01288\t accuracy: 0.5945\n",
      "Epoch: 407\t Train loss: 0.00503\t accuracy: 0.783\t Validation loss: 0.01296\t accuracy: 0.595\n",
      "Epoch: 408\t Train loss: 0.005141\t accuracy: 0.7773\t Validation loss: 0.01277\t accuracy: 0.6014\n",
      "Epoch: 409\t Train loss: 0.005234\t accuracy: 0.7759\t Validation loss: 0.01241\t accuracy: 0.6033\n",
      "Epoch: 410\t Train loss: 0.004983\t accuracy: 0.7777\t Validation loss: 0.0131\t accuracy: 0.5931\n",
      "Epoch: 411\t Train loss: 0.004851\t accuracy: 0.7908\t Validation loss: 0.01303\t accuracy: 0.5935\n",
      "Epoch: 412\t Train loss: 0.005004\t accuracy: 0.7834\t Validation loss: 0.01283\t accuracy: 0.6014\n",
      "Epoch: 413\t Train loss: 0.004927\t accuracy: 0.7915\t Validation loss: 0.01281\t accuracy: 0.5925\n",
      "Epoch: 414\t Train loss: 0.005127\t accuracy: 0.7745\t Validation loss: 0.01285\t accuracy: 0.5999\n",
      "Epoch: 415\t Train loss: 0.005068\t accuracy: 0.7866\t Validation loss: 0.01278\t accuracy: 0.5955\n",
      "Epoch: 416\t Train loss: 0.00488\t accuracy: 0.7795\t Validation loss: 0.01296\t accuracy: 0.5808\n",
      "Epoch: 417\t Train loss: 0.004685\t accuracy: 0.7905\t Validation loss: 0.01256\t accuracy: 0.6102\n",
      "Epoch: 418\t Train loss: 0.005087\t accuracy: 0.7745\t Validation loss: 0.01253\t accuracy: 0.6044\n",
      "Epoch: 419\t Train loss: 0.004999\t accuracy: 0.7816\t Validation loss: 0.01278\t accuracy: 0.5945\n",
      "Epoch: 420\t Train loss: 0.004961\t accuracy: 0.783\t Validation loss: 0.01292\t accuracy: 0.5858\n",
      "Epoch: 421\t Train loss: 0.004791\t accuracy: 0.7873\t Validation loss: 0.01284\t accuracy: 0.5984\n",
      "Epoch: 422\t Train loss: 0.005124\t accuracy: 0.7749\t Validation loss: 0.01279\t accuracy: 0.6083\n",
      "Epoch: 423\t Train loss: 0.005088\t accuracy: 0.7834\t Validation loss: 0.01262\t accuracy: 0.6087\n",
      "Epoch: 424\t Train loss: 0.005188\t accuracy: 0.7752\t Validation loss: 0.0128\t accuracy: 0.5906\n",
      "Epoch: 425\t Train loss: 0.004913\t accuracy: 0.7869\t Validation loss: 0.01265\t accuracy: 0.5911\n",
      "Epoch: 426\t Train loss: 0.005076\t accuracy: 0.783\t Validation loss: 0.01303\t accuracy: 0.5823\n",
      "Epoch: 427\t Train loss: 0.004987\t accuracy: 0.7823\t Validation loss: 0.0133\t accuracy: 0.5691\n",
      "Epoch: 428\t Train loss: 0.005171\t accuracy: 0.7731\t Validation loss: 0.01289\t accuracy: 0.5901\n",
      "Epoch: 429\t Train loss: 0.00493\t accuracy: 0.7827\t Validation loss: 0.01275\t accuracy: 0.5789\n",
      "Epoch: 430\t Train loss: 0.004909\t accuracy: 0.7791\t Validation loss: 0.01312\t accuracy: 0.5975\n",
      "Epoch: 431\t Train loss: 0.005026\t accuracy: 0.771\t Validation loss: 0.0129\t accuracy: 0.6053\n",
      "Epoch: 432\t Train loss: 0.004913\t accuracy: 0.7905\t Validation loss: 0.01296\t accuracy: 0.5989\n",
      "Epoch: 433\t Train loss: 0.005028\t accuracy: 0.7685\t Validation loss: 0.01316\t accuracy: 0.5828\n",
      "Epoch: 434\t Train loss: 0.005077\t accuracy: 0.7887\t Validation loss: 0.01269\t accuracy: 0.5921\n",
      "Epoch: 435\t Train loss: 0.004926\t accuracy: 0.7781\t Validation loss: 0.01319\t accuracy: 0.5691\n",
      "Epoch: 436\t Train loss: 0.004875\t accuracy: 0.7976\t Validation loss: 0.01289\t accuracy: 0.6054\n",
      "Epoch: 437\t Train loss: 0.004964\t accuracy: 0.7827\t Validation loss: 0.01273\t accuracy: 0.5906\n",
      "Epoch: 438\t Train loss: 0.005014\t accuracy: 0.7869\t Validation loss: 0.01306\t accuracy: 0.5872\n",
      "Epoch: 439\t Train loss: 0.005197\t accuracy: 0.7759\t Validation loss: 0.01281\t accuracy: 0.5911\n",
      "Epoch: 440\t Train loss: 0.004901\t accuracy: 0.7912\t Validation loss: 0.01301\t accuracy: 0.5857\n",
      "Epoch: 441\t Train loss: 0.004985\t accuracy: 0.7898\t Validation loss: 0.01271\t accuracy: 0.5902\n",
      "Epoch: 442\t Train loss: 0.004934\t accuracy: 0.788\t Validation loss: 0.01309\t accuracy: 0.597\n",
      "Epoch: 443\t Train loss: 0.004893\t accuracy: 0.7841\t Validation loss: 0.0132\t accuracy: 0.5847\n",
      "Epoch: 444\t Train loss: 0.005059\t accuracy: 0.7823\t Validation loss: 0.01282\t accuracy: 0.596\n",
      "Epoch: 445\t Train loss: 0.004838\t accuracy: 0.7827\t Validation loss: 0.01279\t accuracy: 0.6117\n",
      "Epoch: 446\t Train loss: 0.004703\t accuracy: 0.7923\t Validation loss: 0.01288\t accuracy: 0.5807\n",
      "Epoch: 447\t Train loss: 0.004645\t accuracy: 0.8004\t Validation loss: 0.01273\t accuracy: 0.596\n",
      "Epoch: 448\t Train loss: 0.00473\t accuracy: 0.7891\t Validation loss: 0.01321\t accuracy: 0.5779\n",
      "Epoch: 449\t Train loss: 0.005044\t accuracy: 0.783\t Validation loss: 0.01303\t accuracy: 0.5955\n",
      "Epoch: 450\t Train loss: 0.004778\t accuracy: 0.7844\t Validation loss: 0.01242\t accuracy: 0.6044\n",
      "Epoch: 451\t Train loss: 0.004903\t accuracy: 0.7869\t Validation loss: 0.01286\t accuracy: 0.5989\n",
      "Epoch: 452\t Train loss: 0.005027\t accuracy: 0.7844\t Validation loss: 0.01273\t accuracy: 0.5857\n",
      "Epoch: 453\t Train loss: 0.004636\t accuracy: 0.7962\t Validation loss: 0.01257\t accuracy: 0.5971\n",
      "Epoch: 454\t Train loss: 0.004589\t accuracy: 0.8043\t Validation loss: 0.01286\t accuracy: 0.6077\n",
      "Epoch: 455\t Train loss: 0.004603\t accuracy: 0.8029\t Validation loss: 0.01277\t accuracy: 0.6103\n",
      "Epoch: 456\t Train loss: 0.00492\t accuracy: 0.7944\t Validation loss: 0.01319\t accuracy: 0.603\n",
      "Epoch: 457\t Train loss: 0.004513\t accuracy: 0.7969\t Validation loss: 0.01309\t accuracy: 0.5951\n",
      "Epoch: 458\t Train loss: 0.004924\t accuracy: 0.7834\t Validation loss: 0.0131\t accuracy: 0.5892\n",
      "Epoch: 459\t Train loss: 0.004811\t accuracy: 0.7898\t Validation loss: 0.01314\t accuracy: 0.5867\n",
      "Epoch: 460\t Train loss: 0.004753\t accuracy: 0.783\t Validation loss: 0.01272\t accuracy: 0.5832\n",
      "Epoch: 461\t Train loss: 0.005061\t accuracy: 0.7855\t Validation loss: 0.01292\t accuracy: 0.5853\n",
      "Epoch: 462\t Train loss: 0.00495\t accuracy: 0.7852\t Validation loss: 0.01274\t accuracy: 0.5989\n",
      "Epoch: 463\t Train loss: 0.004736\t accuracy: 0.7809\t Validation loss: 0.01253\t accuracy: 0.6068\n",
      "Epoch: 464\t Train loss: 0.004752\t accuracy: 0.7884\t Validation loss: 0.01262\t accuracy: 0.6049\n",
      "Epoch: 465\t Train loss: 0.004776\t accuracy: 0.7944\t Validation loss: 0.0128\t accuracy: 0.6024\n",
      "Epoch: 466\t Train loss: 0.004979\t accuracy: 0.7834\t Validation loss: 0.01295\t accuracy: 0.5916\n",
      "Epoch: 467\t Train loss: 0.005023\t accuracy: 0.7855\t Validation loss: 0.01254\t accuracy: 0.6004\n",
      "Epoch: 468\t Train loss: 0.004814\t accuracy: 0.7812\t Validation loss: 0.01297\t accuracy: 0.6009\n",
      "Epoch: 469\t Train loss: 0.004641\t accuracy: 0.7912\t Validation loss: 0.01317\t accuracy: 0.5871\n",
      "Epoch: 470\t Train loss: 0.004867\t accuracy: 0.7805\t Validation loss: 0.01288\t accuracy: 0.5847\n",
      "Epoch: 471\t Train loss: 0.004819\t accuracy: 0.7834\t Validation loss: 0.01299\t accuracy: 0.5862\n",
      "Epoch: 472\t Train loss: 0.004774\t accuracy: 0.7834\t Validation loss: 0.01328\t accuracy: 0.575\n",
      "Epoch: 473\t Train loss: 0.004726\t accuracy: 0.7848\t Validation loss: 0.01262\t accuracy: 0.5965\n",
      "Epoch: 474\t Train loss: 0.004661\t accuracy: 0.7987\t Validation loss: 0.01306\t accuracy: 0.5882\n",
      "Epoch: 475\t Train loss: 0.004707\t accuracy: 0.7837\t Validation loss: 0.01307\t accuracy: 0.5945\n",
      "Epoch: 476\t Train loss: 0.004659\t accuracy: 0.7859\t Validation loss: 0.01289\t accuracy: 0.594\n",
      "Epoch: 477\t Train loss: 0.004747\t accuracy: 0.7994\t Validation loss: 0.01311\t accuracy: 0.5886\n",
      "Epoch: 478\t Train loss: 0.004747\t accuracy: 0.793\t Validation loss: 0.01274\t accuracy: 0.5984\n",
      "Epoch: 479\t Train loss: 0.004726\t accuracy: 0.7862\t Validation loss: 0.01256\t accuracy: 0.5936\n",
      "Epoch: 480\t Train loss: 0.004778\t accuracy: 0.7894\t Validation loss: 0.01252\t accuracy: 0.6028\n",
      "Epoch: 481\t Train loss: 0.004772\t accuracy: 0.7901\t Validation loss: 0.01263\t accuracy: 0.5965\n",
      "Epoch: 482\t Train loss: 0.004923\t accuracy: 0.7788\t Validation loss: 0.01251\t accuracy: 0.5891\n",
      "Epoch: 483\t Train loss: 0.004545\t accuracy: 0.8029\t Validation loss: 0.01273\t accuracy: 0.5916\n",
      "Epoch: 484\t Train loss: 0.004607\t accuracy: 0.7937\t Validation loss: 0.01294\t accuracy: 0.5784\n",
      "Epoch: 485\t Train loss: 0.004751\t accuracy: 0.7908\t Validation loss: 0.01241\t accuracy: 0.6185\n",
      "Epoch: 486\t Train loss: 0.004823\t accuracy: 0.7958\t Validation loss: 0.01284\t accuracy: 0.6058\n",
      "Epoch: 487\t Train loss: 0.004784\t accuracy: 0.7976\t Validation loss: 0.01305\t accuracy: 0.6108\n",
      "Epoch: 488\t Train loss: 0.004671\t accuracy: 0.7962\t Validation loss: 0.0126\t accuracy: 0.598\n",
      "Epoch: 489\t Train loss: 0.004649\t accuracy: 0.8022\t Validation loss: 0.01256\t accuracy: 0.5946\n",
      "Epoch: 490\t Train loss: 0.004695\t accuracy: 0.799\t Validation loss: 0.01257\t accuracy: 0.6147\n",
      "Epoch: 491\t Train loss: 0.004883\t accuracy: 0.7912\t Validation loss: 0.0128\t accuracy: 0.5936\n",
      "Epoch: 492\t Train loss: 0.00481\t accuracy: 0.7926\t Validation loss: 0.01295\t accuracy: 0.5902\n",
      "Epoch: 493\t Train loss: 0.004625\t accuracy: 0.8107\t Validation loss: 0.01272\t accuracy: 0.5911\n",
      "Epoch: 494\t Train loss: 0.004466\t accuracy: 0.7983\t Validation loss: 0.01313\t accuracy: 0.5868\n",
      "Epoch: 495\t Train loss: 0.00484\t accuracy: 0.7894\t Validation loss: 0.01286\t accuracy: 0.5981\n",
      "Epoch: 496\t Train loss: 0.004672\t accuracy: 0.7894\t Validation loss: 0.0128\t accuracy: 0.5945\n",
      "Epoch: 497\t Train loss: 0.00466\t accuracy: 0.7947\t Validation loss: 0.01281\t accuracy: 0.5941\n",
      "Epoch: 498\t Train loss: 0.004459\t accuracy: 0.8004\t Validation loss: 0.01312\t accuracy: 0.5863\n",
      "Epoch: 499\t Train loss: 0.004377\t accuracy: 0.8011\t Validation loss: 0.01294\t accuracy: 0.5863\n",
      "Epoch: 500\t Train loss: 0.004695\t accuracy: 0.7915\t Validation loss: 0.01287\t accuracy: 0.5931\n",
      "Epoch: 501\t Train loss: 0.004598\t accuracy: 0.7866\t Validation loss: 0.01281\t accuracy: 0.5808\n",
      "Epoch: 502\t Train loss: 0.004761\t accuracy: 0.7912\t Validation loss: 0.0127\t accuracy: 0.596\n",
      "Epoch: 503\t Train loss: 0.004798\t accuracy: 0.794\t Validation loss: 0.01284\t accuracy: 0.5926\n",
      "Epoch: 504\t Train loss: 0.004749\t accuracy: 0.7891\t Validation loss: 0.01314\t accuracy: 0.5969\n",
      "Epoch: 505\t Train loss: 0.004777\t accuracy: 0.8011\t Validation loss: 0.01299\t accuracy: 0.5793\n",
      "Epoch: 506\t Train loss: 0.004635\t accuracy: 0.7962\t Validation loss: 0.01262\t accuracy: 0.5906\n",
      "Epoch: 507\t Train loss: 0.004761\t accuracy: 0.7855\t Validation loss: 0.0128\t accuracy: 0.5955\n",
      "Epoch: 508\t Train loss: 0.004693\t accuracy: 0.8001\t Validation loss: 0.01304\t accuracy: 0.5945\n",
      "Epoch: 509\t Train loss: 0.004597\t accuracy: 0.7958\t Validation loss: 0.01279\t accuracy: 0.6009\n",
      "Epoch: 510\t Train loss: 0.004778\t accuracy: 0.8015\t Validation loss: 0.0129\t accuracy: 0.6029\n",
      "Epoch: 511\t Train loss: 0.004475\t accuracy: 0.81\t Validation loss: 0.01295\t accuracy: 0.5813\n",
      "Epoch: 512\t Train loss: 0.004432\t accuracy: 0.8033\t Validation loss: 0.01282\t accuracy: 0.6028\n",
      "Epoch: 513\t Train loss: 0.004562\t accuracy: 0.7976\t Validation loss: 0.01282\t accuracy: 0.5989\n",
      "Epoch: 514\t Train loss: 0.004747\t accuracy: 0.7994\t Validation loss: 0.01288\t accuracy: 0.596\n",
      "Epoch: 515\t Train loss: 0.004605\t accuracy: 0.7972\t Validation loss: 0.01292\t accuracy: 0.6048\n",
      "Epoch: 516\t Train loss: 0.004707\t accuracy: 0.7923\t Validation loss: 0.01284\t accuracy: 0.6063\n",
      "Epoch: 517\t Train loss: 0.004529\t accuracy: 0.8001\t Validation loss: 0.01306\t accuracy: 0.6024\n",
      "Epoch: 518\t Train loss: 0.004679\t accuracy: 0.7937\t Validation loss: 0.01291\t accuracy: 0.6014\n",
      "Epoch: 519\t Train loss: 0.00497\t accuracy: 0.7901\t Validation loss: 0.01304\t accuracy: 0.596\n",
      "Epoch: 520\t Train loss: 0.004492\t accuracy: 0.8036\t Validation loss: 0.01319\t accuracy: 0.593\n",
      "Epoch: 521\t Train loss: 0.00457\t accuracy: 0.8043\t Validation loss: 0.01295\t accuracy: 0.5964\n",
      "Epoch: 522\t Train loss: 0.004766\t accuracy: 0.7947\t Validation loss: 0.0128\t accuracy: 0.5853\n",
      "Epoch: 523\t Train loss: 0.004631\t accuracy: 0.7866\t Validation loss: 0.01313\t accuracy: 0.5814\n",
      "Epoch: 524\t Train loss: 0.004361\t accuracy: 0.8168\t Validation loss: 0.01322\t accuracy: 0.5739\n",
      "Epoch: 525\t Train loss: 0.004407\t accuracy: 0.804\t Validation loss: 0.01336\t accuracy: 0.5775\n",
      "Epoch: 526\t Train loss: 0.00453\t accuracy: 0.7933\t Validation loss: 0.01291\t accuracy: 0.5843\n",
      "Epoch: 527\t Train loss: 0.004611\t accuracy: 0.793\t Validation loss: 0.01291\t accuracy: 0.5951\n",
      "Epoch: 528\t Train loss: 0.00445\t accuracy: 0.8026\t Validation loss: 0.01319\t accuracy: 0.5768\n",
      "Epoch: 529\t Train loss: 0.004731\t accuracy: 0.7912\t Validation loss: 0.01301\t accuracy: 0.5901\n",
      "Epoch: 530\t Train loss: 0.00445\t accuracy: 0.8086\t Validation loss: 0.01291\t accuracy: 0.5887\n",
      "Epoch: 531\t Train loss: 0.004757\t accuracy: 0.7887\t Validation loss: 0.01293\t accuracy: 0.5691\n",
      "Epoch: 532\t Train loss: 0.004487\t accuracy: 0.8033\t Validation loss: 0.01286\t accuracy: 0.5945\n",
      "Epoch: 533\t Train loss: 0.004647\t accuracy: 0.7937\t Validation loss: 0.01271\t accuracy: 0.5862\n",
      "Epoch: 534\t Train loss: 0.004622\t accuracy: 0.7994\t Validation loss: 0.01309\t accuracy: 0.5887\n",
      "Epoch: 535\t Train loss: 0.004422\t accuracy: 0.8043\t Validation loss: 0.01278\t accuracy: 0.5833\n",
      "Epoch: 536\t Train loss: 0.004574\t accuracy: 0.8011\t Validation loss: 0.01252\t accuracy: 0.6068\n",
      "Epoch: 537\t Train loss: 0.004575\t accuracy: 0.8001\t Validation loss: 0.01291\t accuracy: 0.5818\n",
      "Epoch: 538\t Train loss: 0.004611\t accuracy: 0.7944\t Validation loss: 0.01303\t accuracy: 0.5807\n",
      "Epoch: 539\t Train loss: 0.00469\t accuracy: 0.7944\t Validation loss: 0.0127\t accuracy: 0.5999\n",
      "Epoch: 540\t Train loss: 0.004416\t accuracy: 0.8061\t Validation loss: 0.01308\t accuracy: 0.5818\n",
      "Epoch: 541\t Train loss: 0.004344\t accuracy: 0.8157\t Validation loss: 0.01283\t accuracy: 0.597\n",
      "Epoch: 542\t Train loss: 0.004572\t accuracy: 0.8061\t Validation loss: 0.0129\t accuracy: 0.5863\n",
      "Epoch: 543\t Train loss: 0.004692\t accuracy: 0.7887\t Validation loss: 0.01296\t accuracy: 0.5872\n",
      "Epoch: 544\t Train loss: 0.004636\t accuracy: 0.7972\t Validation loss: 0.01244\t accuracy: 0.5891\n",
      "Epoch: 545\t Train loss: 0.004617\t accuracy: 0.7994\t Validation loss: 0.01279\t accuracy: 0.5843\n",
      "Epoch: 546\t Train loss: 0.004577\t accuracy: 0.799\t Validation loss: 0.01288\t accuracy: 0.5862\n",
      "Epoch: 547\t Train loss: 0.004612\t accuracy: 0.8054\t Validation loss: 0.01259\t accuracy: 0.5871\n",
      "Epoch: 548\t Train loss: 0.004628\t accuracy: 0.8008\t Validation loss: 0.01261\t accuracy: 0.6072\n",
      "Epoch: 549\t Train loss: 0.004388\t accuracy: 0.8114\t Validation loss: 0.01255\t accuracy: 0.5945\n",
      "Epoch: 550\t Train loss: 0.00432\t accuracy: 0.8143\t Validation loss: 0.01332\t accuracy: 0.5769\n",
      "Epoch: 551\t Train loss: 0.0047\t accuracy: 0.8097\t Validation loss: 0.01267\t accuracy: 0.5814\n",
      "Epoch: 552\t Train loss: 0.004537\t accuracy: 0.8029\t Validation loss: 0.01252\t accuracy: 0.5921\n",
      "Epoch: 553\t Train loss: 0.004494\t accuracy: 0.8072\t Validation loss: 0.01258\t accuracy: 0.5813\n",
      "Epoch: 554\t Train loss: 0.004419\t accuracy: 0.8093\t Validation loss: 0.01229\t accuracy: 0.598\n",
      "Epoch: 555\t Train loss: 0.00467\t accuracy: 0.7908\t Validation loss: 0.01251\t accuracy: 0.6024\n",
      "Epoch: 556\t Train loss: 0.004457\t accuracy: 0.8026\t Validation loss: 0.01283\t accuracy: 0.5832\n",
      "Epoch: 557\t Train loss: 0.004404\t accuracy: 0.8129\t Validation loss: 0.01274\t accuracy: 0.5783\n",
      "Epoch: 558\t Train loss: 0.004593\t accuracy: 0.8068\t Validation loss: 0.01268\t accuracy: 0.5936\n",
      "Epoch: 559\t Train loss: 0.004772\t accuracy: 0.7894\t Validation loss: 0.01277\t accuracy: 0.5935\n",
      "Epoch: 560\t Train loss: 0.004742\t accuracy: 0.7912\t Validation loss: 0.01265\t accuracy: 0.5949\n",
      "Epoch: 561\t Train loss: 0.004632\t accuracy: 0.7923\t Validation loss: 0.01293\t accuracy: 0.5955\n",
      "Epoch: 562\t Train loss: 0.004632\t accuracy: 0.7969\t Validation loss: 0.01255\t accuracy: 0.6009\n",
      "Epoch: 563\t Train loss: 0.004766\t accuracy: 0.8008\t Validation loss: 0.01319\t accuracy: 0.5691\n",
      "Epoch: 564\t Train loss: 0.004658\t accuracy: 0.8033\t Validation loss: 0.01303\t accuracy: 0.5975\n",
      "Epoch: 565\t Train loss: 0.004554\t accuracy: 0.7979\t Validation loss: 0.01261\t accuracy: 0.5925\n",
      "Epoch: 566\t Train loss: 0.004631\t accuracy: 0.7976\t Validation loss: 0.01297\t accuracy: 0.5906\n",
      "Epoch: 567\t Train loss: 0.004583\t accuracy: 0.8036\t Validation loss: 0.01308\t accuracy: 0.5847\n",
      "Epoch: 568\t Train loss: 0.004528\t accuracy: 0.804\t Validation loss: 0.01287\t accuracy: 0.5877\n",
      "Epoch: 569\t Train loss: 0.004474\t accuracy: 0.799\t Validation loss: 0.01318\t accuracy: 0.5726\n",
      "Epoch: 570\t Train loss: 0.004219\t accuracy: 0.8153\t Validation loss: 0.01305\t accuracy: 0.597\n",
      "Epoch: 571\t Train loss: 0.00454\t accuracy: 0.7994\t Validation loss: 0.0132\t accuracy: 0.5813\n",
      "Epoch: 572\t Train loss: 0.004696\t accuracy: 0.799\t Validation loss: 0.01271\t accuracy: 0.6028\n",
      "Epoch: 573\t Train loss: 0.004849\t accuracy: 0.7848\t Validation loss: 0.01278\t accuracy: 0.6091\n",
      "Epoch: 574\t Train loss: 0.004553\t accuracy: 0.8054\t Validation loss: 0.0127\t accuracy: 0.594\n",
      "Epoch: 575\t Train loss: 0.00456\t accuracy: 0.8065\t Validation loss: 0.01299\t accuracy: 0.5878\n",
      "Epoch: 576\t Train loss: 0.004381\t accuracy: 0.8043\t Validation loss: 0.01274\t accuracy: 0.5955\n",
      "Epoch: 577\t Train loss: 0.004106\t accuracy: 0.8303\t Validation loss: 0.01317\t accuracy: 0.5872\n",
      "Epoch: 578\t Train loss: 0.004355\t accuracy: 0.8065\t Validation loss: 0.01309\t accuracy: 0.5887\n",
      "Epoch: 579\t Train loss: 0.004303\t accuracy: 0.8168\t Validation loss: 0.01301\t accuracy: 0.5984\n",
      "Epoch: 580\t Train loss: 0.004268\t accuracy: 0.815\t Validation loss: 0.01321\t accuracy: 0.593\n",
      "Epoch: 581\t Train loss: 0.004442\t accuracy: 0.8072\t Validation loss: 0.0133\t accuracy: 0.6038\n",
      "Epoch: 582\t Train loss: 0.004347\t accuracy: 0.815\t Validation loss: 0.01318\t accuracy: 0.5858\n",
      "Epoch: 583\t Train loss: 0.00455\t accuracy: 0.8047\t Validation loss: 0.01293\t accuracy: 0.5979\n",
      "Epoch: 584\t Train loss: 0.004626\t accuracy: 0.8089\t Validation loss: 0.01327\t accuracy: 0.5959\n",
      "Epoch: 585\t Train loss: 0.004301\t accuracy: 0.815\t Validation loss: 0.01302\t accuracy: 0.5852\n",
      "Epoch: 586\t Train loss: 0.004494\t accuracy: 0.8004\t Validation loss: 0.01374\t accuracy: 0.573\n",
      "Epoch: 587\t Train loss: 0.004625\t accuracy: 0.8001\t Validation loss: 0.01326\t accuracy: 0.5935\n",
      "Epoch: 588\t Train loss: 0.004548\t accuracy: 0.7976\t Validation loss: 0.01305\t accuracy: 0.5955\n",
      "Epoch: 589\t Train loss: 0.004584\t accuracy: 0.7997\t Validation loss: 0.01292\t accuracy: 0.5924\n",
      "Epoch: 590\t Train loss: 0.004445\t accuracy: 0.8118\t Validation loss: 0.01296\t accuracy: 0.5993\n",
      "Epoch: 591\t Train loss: 0.004288\t accuracy: 0.82\t Validation loss: 0.01286\t accuracy: 0.5951\n",
      "Epoch: 592\t Train loss: 0.00441\t accuracy: 0.8068\t Validation loss: 0.01325\t accuracy: 0.5886\n",
      "Epoch: 593\t Train loss: 0.004591\t accuracy: 0.7958\t Validation loss: 0.01304\t accuracy: 0.5965\n",
      "Epoch: 594\t Train loss: 0.004438\t accuracy: 0.8079\t Validation loss: 0.01299\t accuracy: 0.596\n",
      "Epoch: 595\t Train loss: 0.004855\t accuracy: 0.7947\t Validation loss: 0.01302\t accuracy: 0.5926\n",
      "Epoch: 596\t Train loss: 0.004698\t accuracy: 0.804\t Validation loss: 0.01301\t accuracy: 0.5779\n",
      "Epoch: 597\t Train loss: 0.004323\t accuracy: 0.8139\t Validation loss: 0.01313\t accuracy: 0.5832\n",
      "Epoch: 598\t Train loss: 0.004513\t accuracy: 0.8001\t Validation loss: 0.01257\t accuracy: 0.6077\n",
      "Epoch: 599\t Train loss: 0.004478\t accuracy: 0.8107\t Validation loss: 0.01315\t accuracy: 0.5911\n",
      "Epoch: 600\t Train loss: 0.004216\t accuracy: 0.8111\t Validation loss: 0.01287\t accuracy: 0.5827\n",
      "Epoch: 601\t Train loss: 0.004336\t accuracy: 0.8164\t Validation loss: 0.01283\t accuracy: 0.5872\n",
      "Epoch: 602\t Train loss: 0.004526\t accuracy: 0.81\t Validation loss: 0.01269\t accuracy: 0.5945\n",
      "Epoch: 603\t Train loss: 0.004565\t accuracy: 0.804\t Validation loss: 0.01326\t accuracy: 0.5871\n",
      "Epoch: 604\t Train loss: 0.00429\t accuracy: 0.8157\t Validation loss: 0.01295\t accuracy: 0.6033\n",
      "Epoch: 605\t Train loss: 0.004431\t accuracy: 0.8107\t Validation loss: 0.01288\t accuracy: 0.5847\n",
      "Epoch: 606\t Train loss: 0.004251\t accuracy: 0.8207\t Validation loss: 0.01316\t accuracy: 0.5724\n",
      "Epoch: 607\t Train loss: 0.004626\t accuracy: 0.7979\t Validation loss: 0.01284\t accuracy: 0.5877\n",
      "Epoch: 608\t Train loss: 0.004332\t accuracy: 0.8125\t Validation loss: 0.01311\t accuracy: 0.5803\n",
      "Epoch: 609\t Train loss: 0.004341\t accuracy: 0.8139\t Validation loss: 0.01284\t accuracy: 0.6052\n",
      "Epoch: 610\t Train loss: 0.004433\t accuracy: 0.8153\t Validation loss: 0.01319\t accuracy: 0.5823\n",
      "Epoch: 611\t Train loss: 0.004171\t accuracy: 0.8171\t Validation loss: 0.01276\t accuracy: 0.5949\n",
      "Epoch: 612\t Train loss: 0.004251\t accuracy: 0.8178\t Validation loss: 0.01284\t accuracy: 0.5857\n",
      "Epoch: 613\t Train loss: 0.004353\t accuracy: 0.815\t Validation loss: 0.0128\t accuracy: 0.5744\n",
      "Epoch: 614\t Train loss: 0.004162\t accuracy: 0.8253\t Validation loss: 0.01293\t accuracy: 0.5851\n",
      "Epoch: 615\t Train loss: 0.004448\t accuracy: 0.8061\t Validation loss: 0.01298\t accuracy: 0.5916\n",
      "Epoch: 616\t Train loss: 0.004425\t accuracy: 0.8022\t Validation loss: 0.01292\t accuracy: 0.5978\n",
      "Epoch: 617\t Train loss: 0.004338\t accuracy: 0.8125\t Validation loss: 0.01321\t accuracy: 0.5911\n",
      "Epoch: 618\t Train loss: 0.004217\t accuracy: 0.8185\t Validation loss: 0.01315\t accuracy: 0.5925\n",
      "Epoch: 619\t Train loss: 0.004293\t accuracy: 0.8161\t Validation loss: 0.01301\t accuracy: 0.6018\n",
      "Epoch: 620\t Train loss: 0.004279\t accuracy: 0.8125\t Validation loss: 0.01338\t accuracy: 0.5842\n",
      "Epoch: 621\t Train loss: 0.004478\t accuracy: 0.8079\t Validation loss: 0.01294\t accuracy: 0.5906\n",
      "Epoch: 622\t Train loss: 0.004389\t accuracy: 0.8029\t Validation loss: 0.01293\t accuracy: 0.595\n",
      "Epoch: 623\t Train loss: 0.00432\t accuracy: 0.8107\t Validation loss: 0.01276\t accuracy: 0.5817\n",
      "Epoch: 624\t Train loss: 0.004554\t accuracy: 0.8011\t Validation loss: 0.01285\t accuracy: 0.5851\n",
      "Epoch: 625\t Train loss: 0.004428\t accuracy: 0.8082\t Validation loss: 0.01288\t accuracy: 0.5897\n",
      "Epoch: 626\t Train loss: 0.004579\t accuracy: 0.7866\t Validation loss: 0.01303\t accuracy: 0.5921\n",
      "Epoch: 627\t Train loss: 0.00438\t accuracy: 0.8079\t Validation loss: 0.01284\t accuracy: 0.5857\n",
      "Epoch: 628\t Train loss: 0.004464\t accuracy: 0.8114\t Validation loss: 0.01251\t accuracy: 0.5999\n",
      "Epoch: 629\t Train loss: 0.00428\t accuracy: 0.8178\t Validation loss: 0.01274\t accuracy: 0.5969\n",
      "Epoch: 630\t Train loss: 0.004173\t accuracy: 0.8168\t Validation loss: 0.01324\t accuracy: 0.5877\n",
      "Epoch: 631\t Train loss: 0.004359\t accuracy: 0.804\t Validation loss: 0.01267\t accuracy: 0.5969\n",
      "Epoch: 632\t Train loss: 0.004384\t accuracy: 0.8185\t Validation loss: 0.0128\t accuracy: 0.5886\n",
      "Epoch: 633\t Train loss: 0.004402\t accuracy: 0.8161\t Validation loss: 0.0128\t accuracy: 0.5798\n",
      "Epoch: 634\t Train loss: 0.004245\t accuracy: 0.8143\t Validation loss: 0.01295\t accuracy: 0.5847\n",
      "Epoch: 635\t Train loss: 0.004425\t accuracy: 0.8107\t Validation loss: 0.01297\t accuracy: 0.5817\n",
      "Epoch: 636\t Train loss: 0.004346\t accuracy: 0.8047\t Validation loss: 0.01254\t accuracy: 0.594\n",
      "Epoch: 637\t Train loss: 0.004456\t accuracy: 0.8089\t Validation loss: 0.01281\t accuracy: 0.5851\n",
      "Epoch: 638\t Train loss: 0.004278\t accuracy: 0.815\t Validation loss: 0.01291\t accuracy: 0.6008\n",
      "Epoch: 639\t Train loss: 0.004436\t accuracy: 0.8072\t Validation loss: 0.01282\t accuracy: 0.5881\n",
      "Epoch: 640\t Train loss: 0.004383\t accuracy: 0.8161\t Validation loss: 0.01296\t accuracy: 0.5984\n",
      "Epoch: 641\t Train loss: 0.00444\t accuracy: 0.8043\t Validation loss: 0.0125\t accuracy: 0.6023\n",
      "Epoch: 642\t Train loss: 0.004296\t accuracy: 0.8196\t Validation loss: 0.01327\t accuracy: 0.5896\n",
      "Epoch: 643\t Train loss: 0.004513\t accuracy: 0.8061\t Validation loss: 0.01327\t accuracy: 0.5989\n",
      "Epoch: 644\t Train loss: 0.004343\t accuracy: 0.8129\t Validation loss: 0.01294\t accuracy: 0.5945\n",
      "Epoch: 645\t Train loss: 0.004232\t accuracy: 0.8161\t Validation loss: 0.0128\t accuracy: 0.5964\n",
      "Epoch: 646\t Train loss: 0.004144\t accuracy: 0.8192\t Validation loss: 0.01299\t accuracy: 0.5851\n",
      "Epoch: 647\t Train loss: 0.004232\t accuracy: 0.815\t Validation loss: 0.01297\t accuracy: 0.5964\n",
      "Epoch: 648\t Train loss: 0.004322\t accuracy: 0.8093\t Validation loss: 0.01274\t accuracy: 0.6043\n",
      "Epoch: 649\t Train loss: 0.003997\t accuracy: 0.8239\t Validation loss: 0.01271\t accuracy: 0.5901\n",
      "Epoch: 650\t Train loss: 0.004219\t accuracy: 0.8114\t Validation loss: 0.0128\t accuracy: 0.5984\n",
      "Epoch: 651\t Train loss: 0.004165\t accuracy: 0.8189\t Validation loss: 0.0125\t accuracy: 0.592\n",
      "Epoch: 652\t Train loss: 0.004378\t accuracy: 0.8121\t Validation loss: 0.01265\t accuracy: 0.6027\n",
      "Epoch: 653\t Train loss: 0.004263\t accuracy: 0.8185\t Validation loss: 0.01297\t accuracy: 0.6013\n",
      "Epoch: 654\t Train loss: 0.004478\t accuracy: 0.8018\t Validation loss: 0.01272\t accuracy: 0.6096\n",
      "Epoch: 655\t Train loss: 0.004106\t accuracy: 0.821\t Validation loss: 0.013\t accuracy: 0.5936\n",
      "Epoch: 656\t Train loss: 0.004395\t accuracy: 0.8065\t Validation loss: 0.01269\t accuracy: 0.5999\n",
      "Epoch: 657\t Train loss: 0.004321\t accuracy: 0.8072\t Validation loss: 0.01358\t accuracy: 0.5719\n",
      "Epoch: 658\t Train loss: 0.004385\t accuracy: 0.8139\t Validation loss: 0.01278\t accuracy: 0.6087\n",
      "Epoch: 659\t Train loss: 0.00438\t accuracy: 0.8189\t Validation loss: 0.01313\t accuracy: 0.5788\n",
      "Epoch: 660\t Train loss: 0.004181\t accuracy: 0.8153\t Validation loss: 0.01284\t accuracy: 0.595\n",
      "Epoch: 661\t Train loss: 0.004361\t accuracy: 0.8058\t Validation loss: 0.0128\t accuracy: 0.594\n",
      "Epoch: 662\t Train loss: 0.004268\t accuracy: 0.815\t Validation loss: 0.01266\t accuracy: 0.6087\n",
      "Epoch: 663\t Train loss: 0.004345\t accuracy: 0.8164\t Validation loss: 0.01268\t accuracy: 0.597\n",
      "Epoch: 664\t Train loss: 0.004408\t accuracy: 0.8043\t Validation loss: 0.0128\t accuracy: 0.5921\n",
      "Epoch: 665\t Train loss: 0.004485\t accuracy: 0.8118\t Validation loss: 0.01275\t accuracy: 0.6081\n",
      "Epoch: 666\t Train loss: 0.004199\t accuracy: 0.8189\t Validation loss: 0.01252\t accuracy: 0.5999\n",
      "Epoch: 667\t Train loss: 0.004356\t accuracy: 0.8058\t Validation loss: 0.01255\t accuracy: 0.613\n",
      "Epoch: 668\t Train loss: 0.00427\t accuracy: 0.8192\t Validation loss: 0.01312\t accuracy: 0.5891\n",
      "Epoch: 669\t Train loss: 0.004097\t accuracy: 0.8221\t Validation loss: 0.01252\t accuracy: 0.6218\n",
      "Epoch: 670\t Train loss: 0.004182\t accuracy: 0.8114\t Validation loss: 0.01274\t accuracy: 0.6131\n",
      "Epoch: 671\t Train loss: 0.004114\t accuracy: 0.8182\t Validation loss: 0.01262\t accuracy: 0.6037\n",
      "Epoch: 672\t Train loss: 0.004254\t accuracy: 0.8161\t Validation loss: 0.01238\t accuracy: 0.6009\n",
      "Epoch: 673\t Train loss: 0.004359\t accuracy: 0.8111\t Validation loss: 0.01276\t accuracy: 0.6086\n",
      "Epoch: 674\t Train loss: 0.004007\t accuracy: 0.8221\t Validation loss: 0.01256\t accuracy: 0.5994\n",
      "Epoch: 675\t Train loss: 0.003978\t accuracy: 0.8239\t Validation loss: 0.013\t accuracy: 0.5945\n",
      "Epoch: 676\t Train loss: 0.004184\t accuracy: 0.8143\t Validation loss: 0.01263\t accuracy: 0.6028\n",
      "Epoch: 677\t Train loss: 0.004187\t accuracy: 0.8246\t Validation loss: 0.01277\t accuracy: 0.5959\n",
      "Epoch: 678\t Train loss: 0.004318\t accuracy: 0.8093\t Validation loss: 0.01316\t accuracy: 0.5857\n",
      "Epoch: 679\t Train loss: 0.004415\t accuracy: 0.8089\t Validation loss: 0.01281\t accuracy: 0.5994\n",
      "Epoch: 680\t Train loss: 0.004254\t accuracy: 0.8164\t Validation loss: 0.01274\t accuracy: 0.5911\n",
      "Epoch: 681\t Train loss: 0.003975\t accuracy: 0.8221\t Validation loss: 0.01289\t accuracy: 0.6028\n",
      "Epoch: 682\t Train loss: 0.004238\t accuracy: 0.8192\t Validation loss: 0.01299\t accuracy: 0.5901\n",
      "Epoch: 683\t Train loss: 0.004383\t accuracy: 0.8153\t Validation loss: 0.013\t accuracy: 0.5989\n",
      "Epoch: 684\t Train loss: 0.003968\t accuracy: 0.8317\t Validation loss: 0.01259\t accuracy: 0.5901\n",
      "Epoch: 685\t Train loss: 0.004217\t accuracy: 0.8143\t Validation loss: 0.0128\t accuracy: 0.5896\n",
      "Epoch: 686\t Train loss: 0.004129\t accuracy: 0.8246\t Validation loss: 0.01287\t accuracy: 0.6033\n",
      "Epoch: 687\t Train loss: 0.004075\t accuracy: 0.8203\t Validation loss: 0.01268\t accuracy: 0.6067\n",
      "Epoch: 688\t Train loss: 0.004091\t accuracy: 0.8256\t Validation loss: 0.01292\t accuracy: 0.5974\n",
      "Epoch: 689\t Train loss: 0.004314\t accuracy: 0.8072\t Validation loss: 0.01267\t accuracy: 0.6052\n",
      "Epoch: 690\t Train loss: 0.004278\t accuracy: 0.8164\t Validation loss: 0.0128\t accuracy: 0.5945\n",
      "Epoch: 691\t Train loss: 0.004311\t accuracy: 0.8168\t Validation loss: 0.01261\t accuracy: 0.6018\n",
      "Epoch: 692\t Train loss: 0.004156\t accuracy: 0.8221\t Validation loss: 0.01286\t accuracy: 0.5959\n",
      "Epoch: 693\t Train loss: 0.004186\t accuracy: 0.8164\t Validation loss: 0.01295\t accuracy: 0.6082\n",
      "Epoch: 694\t Train loss: 0.004341\t accuracy: 0.8136\t Validation loss: 0.01283\t accuracy: 0.6038\n",
      "Epoch: 695\t Train loss: 0.004226\t accuracy: 0.8242\t Validation loss: 0.01315\t accuracy: 0.5828\n",
      "Epoch: 696\t Train loss: 0.004242\t accuracy: 0.8249\t Validation loss: 0.01294\t accuracy: 0.5988\n",
      "Epoch: 697\t Train loss: 0.004251\t accuracy: 0.8139\t Validation loss: 0.013\t accuracy: 0.5925\n",
      "Epoch: 698\t Train loss: 0.004109\t accuracy: 0.8224\t Validation loss: 0.01334\t accuracy: 0.5862\n",
      "Epoch: 699\t Train loss: 0.004314\t accuracy: 0.8104\t Validation loss: 0.01264\t accuracy: 0.6097\n",
      "Epoch: 700\t Train loss: 0.004352\t accuracy: 0.8136\t Validation loss: 0.01281\t accuracy: 0.6009\n",
      "Epoch: 701\t Train loss: 0.004183\t accuracy: 0.8313\t Validation loss: 0.01294\t accuracy: 0.6013\n",
      "Epoch: 702\t Train loss: 0.004349\t accuracy: 0.8072\t Validation loss: 0.01264\t accuracy: 0.593\n",
      "Epoch: 703\t Train loss: 0.004241\t accuracy: 0.8129\t Validation loss: 0.01324\t accuracy: 0.5866\n",
      "Epoch: 704\t Train loss: 0.004425\t accuracy: 0.8089\t Validation loss: 0.01303\t accuracy: 0.5842\n",
      "Epoch: 705\t Train loss: 0.004258\t accuracy: 0.8107\t Validation loss: 0.01306\t accuracy: 0.5925\n",
      "Epoch: 706\t Train loss: 0.004201\t accuracy: 0.8242\t Validation loss: 0.01334\t accuracy: 0.5944\n",
      "Epoch: 707\t Train loss: 0.003907\t accuracy: 0.8299\t Validation loss: 0.01288\t accuracy: 0.5989\n",
      "Epoch: 708\t Train loss: 0.003947\t accuracy: 0.8324\t Validation loss: 0.01287\t accuracy: 0.6037\n",
      "Epoch: 709\t Train loss: 0.004067\t accuracy: 0.8196\t Validation loss: 0.01345\t accuracy: 0.5862\n",
      "Epoch: 710\t Train loss: 0.004199\t accuracy: 0.8178\t Validation loss: 0.01283\t accuracy: 0.5959\n",
      "Epoch: 711\t Train loss: 0.004069\t accuracy: 0.82\t Validation loss: 0.01304\t accuracy: 0.5936\n",
      "Epoch: 712\t Train loss: 0.003982\t accuracy: 0.832\t Validation loss: 0.01317\t accuracy: 0.5911\n",
      "Epoch: 713\t Train loss: 0.0043\t accuracy: 0.8189\t Validation loss: 0.01305\t accuracy: 0.5794\n",
      "Epoch: 714\t Train loss: 0.004302\t accuracy: 0.8121\t Validation loss: 0.01311\t accuracy: 0.5994\n",
      "Epoch: 715\t Train loss: 0.004144\t accuracy: 0.8189\t Validation loss: 0.01284\t accuracy: 0.6037\n",
      "Epoch: 716\t Train loss: 0.004195\t accuracy: 0.821\t Validation loss: 0.01287\t accuracy: 0.5847\n",
      "Epoch: 717\t Train loss: 0.004134\t accuracy: 0.8239\t Validation loss: 0.0129\t accuracy: 0.6048\n",
      "Epoch: 718\t Train loss: 0.004093\t accuracy: 0.8239\t Validation loss: 0.01281\t accuracy: 0.5905\n",
      "Epoch: 719\t Train loss: 0.004224\t accuracy: 0.8228\t Validation loss: 0.01264\t accuracy: 0.6058\n",
      "Epoch: 720\t Train loss: 0.0043\t accuracy: 0.8136\t Validation loss: 0.01314\t accuracy: 0.5901\n",
      "Epoch: 721\t Train loss: 0.003984\t accuracy: 0.8345\t Validation loss: 0.01282\t accuracy: 0.593\n",
      "Epoch: 722\t Train loss: 0.003984\t accuracy: 0.8306\t Validation loss: 0.01295\t accuracy: 0.5857\n",
      "Epoch: 723\t Train loss: 0.003941\t accuracy: 0.8342\t Validation loss: 0.01309\t accuracy: 0.5871\n",
      "Epoch: 724\t Train loss: 0.004154\t accuracy: 0.8175\t Validation loss: 0.01291\t accuracy: 0.5935\n",
      "Epoch: 725\t Train loss: 0.004214\t accuracy: 0.8253\t Validation loss: 0.01335\t accuracy: 0.5823\n",
      "Epoch: 726\t Train loss: 0.004331\t accuracy: 0.8157\t Validation loss: 0.01299\t accuracy: 0.6037\n",
      "Epoch: 727\t Train loss: 0.004076\t accuracy: 0.8317\t Validation loss: 0.01327\t accuracy: 0.5877\n",
      "Epoch: 728\t Train loss: 0.004126\t accuracy: 0.8228\t Validation loss: 0.01298\t accuracy: 0.5939\n",
      "Epoch: 729\t Train loss: 0.003994\t accuracy: 0.8278\t Validation loss: 0.01301\t accuracy: 0.592\n",
      "Epoch: 730\t Train loss: 0.004108\t accuracy: 0.8303\t Validation loss: 0.01264\t accuracy: 0.5935\n",
      "Epoch: 731\t Train loss: 0.004293\t accuracy: 0.8129\t Validation loss: 0.01277\t accuracy: 0.5998\n",
      "Epoch: 732\t Train loss: 0.004169\t accuracy: 0.8143\t Validation loss: 0.01275\t accuracy: 0.5935\n",
      "Epoch: 733\t Train loss: 0.004238\t accuracy: 0.8263\t Validation loss: 0.0127\t accuracy: 0.5955\n",
      "Epoch: 734\t Train loss: 0.003949\t accuracy: 0.8331\t Validation loss: 0.0126\t accuracy: 0.616\n",
      "Epoch: 735\t Train loss: 0.003984\t accuracy: 0.8253\t Validation loss: 0.01334\t accuracy: 0.5994\n",
      "Epoch: 736\t Train loss: 0.004201\t accuracy: 0.8185\t Validation loss: 0.01289\t accuracy: 0.5857\n",
      "Epoch: 737\t Train loss: 0.003892\t accuracy: 0.8349\t Validation loss: 0.01296\t accuracy: 0.596\n",
      "Epoch: 738\t Train loss: 0.00399\t accuracy: 0.8285\t Validation loss: 0.01278\t accuracy: 0.5989\n",
      "Epoch: 739\t Train loss: 0.003989\t accuracy: 0.8253\t Validation loss: 0.01283\t accuracy: 0.5846\n",
      "Epoch: 740\t Train loss: 0.004145\t accuracy: 0.82\t Validation loss: 0.01265\t accuracy: 0.6092\n",
      "Epoch: 741\t Train loss: 0.003955\t accuracy: 0.8295\t Validation loss: 0.01283\t accuracy: 0.6057\n",
      "Epoch: 742\t Train loss: 0.004088\t accuracy: 0.8299\t Validation loss: 0.01273\t accuracy: 0.5974\n",
      "Epoch: 743\t Train loss: 0.003993\t accuracy: 0.8299\t Validation loss: 0.01289\t accuracy: 0.6096\n",
      "Epoch: 744\t Train loss: 0.004131\t accuracy: 0.8161\t Validation loss: 0.013\t accuracy: 0.6067\n",
      "Epoch: 745\t Train loss: 0.004037\t accuracy: 0.8271\t Validation loss: 0.01272\t accuracy: 0.5944\n",
      "Epoch: 746\t Train loss: 0.004218\t accuracy: 0.8139\t Validation loss: 0.01295\t accuracy: 0.5871\n",
      "Epoch: 747\t Train loss: 0.004223\t accuracy: 0.815\t Validation loss: 0.01281\t accuracy: 0.5935\n",
      "Epoch: 748\t Train loss: 0.003984\t accuracy: 0.8381\t Validation loss: 0.01323\t accuracy: 0.597\n",
      "Epoch: 749\t Train loss: 0.003831\t accuracy: 0.8356\t Validation loss: 0.01297\t accuracy: 0.5925\n",
      "Epoch: 750\t Train loss: 0.004094\t accuracy: 0.8246\t Validation loss: 0.01279\t accuracy: 0.6053\n",
      "Epoch: 751\t Train loss: 0.004084\t accuracy: 0.8185\t Validation loss: 0.01276\t accuracy: 0.6038\n",
      "Epoch: 752\t Train loss: 0.004222\t accuracy: 0.8232\t Validation loss: 0.01289\t accuracy: 0.6018\n",
      "Epoch: 753\t Train loss: 0.004002\t accuracy: 0.8317\t Validation loss: 0.01228\t accuracy: 0.6067\n",
      "Epoch: 754\t Train loss: 0.004033\t accuracy: 0.8349\t Validation loss: 0.01295\t accuracy: 0.5934\n",
      "Epoch: 755\t Train loss: 0.004055\t accuracy: 0.82\t Validation loss: 0.01289\t accuracy: 0.6042\n",
      "Epoch: 756\t Train loss: 0.003948\t accuracy: 0.8327\t Validation loss: 0.01311\t accuracy: 0.6043\n",
      "Epoch: 757\t Train loss: 0.004241\t accuracy: 0.8189\t Validation loss: 0.01303\t accuracy: 0.5861\n",
      "Epoch: 758\t Train loss: 0.004309\t accuracy: 0.8157\t Validation loss: 0.01307\t accuracy: 0.5905\n",
      "Epoch: 759\t Train loss: 0.004061\t accuracy: 0.8253\t Validation loss: 0.0128\t accuracy: 0.5984\n",
      "Epoch: 760\t Train loss: 0.00413\t accuracy: 0.8288\t Validation loss: 0.01282\t accuracy: 0.5959\n",
      "Epoch: 761\t Train loss: 0.004059\t accuracy: 0.8256\t Validation loss: 0.01282\t accuracy: 0.5964\n",
      "Epoch: 762\t Train loss: 0.004146\t accuracy: 0.8221\t Validation loss: 0.01262\t accuracy: 0.5881\n",
      "Epoch: 763\t Train loss: 0.00408\t accuracy: 0.8263\t Validation loss: 0.01263\t accuracy: 0.5993\n",
      "Epoch: 764\t Train loss: 0.004058\t accuracy: 0.8278\t Validation loss: 0.01282\t accuracy: 0.5999\n",
      "Epoch: 765\t Train loss: 0.004153\t accuracy: 0.8168\t Validation loss: 0.01267\t accuracy: 0.5999\n",
      "Epoch: 766\t Train loss: 0.004028\t accuracy: 0.8278\t Validation loss: 0.0129\t accuracy: 0.6057\n",
      "Epoch: 767\t Train loss: 0.004145\t accuracy: 0.8217\t Validation loss: 0.01235\t accuracy: 0.6116\n",
      "Epoch: 768\t Train loss: 0.004123\t accuracy: 0.8242\t Validation loss: 0.01291\t accuracy: 0.6092\n",
      "Epoch: 769\t Train loss: 0.004027\t accuracy: 0.8239\t Validation loss: 0.01262\t accuracy: 0.6013\n",
      "Epoch: 770\t Train loss: 0.004072\t accuracy: 0.8295\t Validation loss: 0.01278\t accuracy: 0.5969\n",
      "Epoch: 771\t Train loss: 0.003987\t accuracy: 0.8285\t Validation loss: 0.0127\t accuracy: 0.6111\n",
      "Epoch: 772\t Train loss: 0.004161\t accuracy: 0.8232\t Validation loss: 0.01277\t accuracy: 0.5896\n",
      "Epoch: 773\t Train loss: 0.004107\t accuracy: 0.8317\t Validation loss: 0.01254\t accuracy: 0.5989\n",
      "Epoch: 774\t Train loss: 0.004043\t accuracy: 0.8203\t Validation loss: 0.01264\t accuracy: 0.5979\n",
      "Epoch: 775\t Train loss: 0.00396\t accuracy: 0.8256\t Validation loss: 0.01265\t accuracy: 0.5944\n",
      "Epoch: 776\t Train loss: 0.003968\t accuracy: 0.8349\t Validation loss: 0.01318\t accuracy: 0.5876\n",
      "Epoch: 777\t Train loss: 0.003975\t accuracy: 0.8232\t Validation loss: 0.01253\t accuracy: 0.6038\n",
      "Epoch: 778\t Train loss: 0.00383\t accuracy: 0.8366\t Validation loss: 0.0127\t accuracy: 0.6004\n",
      "Epoch: 779\t Train loss: 0.004037\t accuracy: 0.8285\t Validation loss: 0.01286\t accuracy: 0.5984\n",
      "Epoch: 780\t Train loss: 0.004048\t accuracy: 0.8271\t Validation loss: 0.01302\t accuracy: 0.5998\n",
      "Epoch: 781\t Train loss: 0.004253\t accuracy: 0.8189\t Validation loss: 0.01253\t accuracy: 0.595\n",
      "Epoch: 782\t Train loss: 0.003932\t accuracy: 0.8313\t Validation loss: 0.01263\t accuracy: 0.6091\n",
      "Epoch: 783\t Train loss: 0.003993\t accuracy: 0.8306\t Validation loss: 0.01272\t accuracy: 0.6052\n",
      "Epoch: 784\t Train loss: 0.003983\t accuracy: 0.8278\t Validation loss: 0.01269\t accuracy: 0.6101\n",
      "Epoch: 785\t Train loss: 0.003903\t accuracy: 0.8381\t Validation loss: 0.01288\t accuracy: 0.5896\n",
      "Epoch: 786\t Train loss: 0.003811\t accuracy: 0.8391\t Validation loss: 0.01271\t accuracy: 0.6023\n",
      "Epoch: 787\t Train loss: 0.004071\t accuracy: 0.8327\t Validation loss: 0.01257\t accuracy: 0.6156\n",
      "Epoch: 788\t Train loss: 0.003824\t accuracy: 0.8356\t Validation loss: 0.01299\t accuracy: 0.6024\n",
      "Epoch: 789\t Train loss: 0.003771\t accuracy: 0.8352\t Validation loss: 0.01275\t accuracy: 0.6048\n",
      "Epoch: 790\t Train loss: 0.003797\t accuracy: 0.8413\t Validation loss: 0.01293\t accuracy: 0.5949\n",
      "Epoch: 791\t Train loss: 0.003926\t accuracy: 0.8317\t Validation loss: 0.01311\t accuracy: 0.5812\n",
      "Epoch: 792\t Train loss: 0.003937\t accuracy: 0.8267\t Validation loss: 0.0126\t accuracy: 0.597\n",
      "Epoch: 793\t Train loss: 0.003938\t accuracy: 0.8335\t Validation loss: 0.01285\t accuracy: 0.6047\n",
      "Epoch: 794\t Train loss: 0.003945\t accuracy: 0.8303\t Validation loss: 0.01264\t accuracy: 0.5974\n",
      "Epoch: 795\t Train loss: 0.004003\t accuracy: 0.8303\t Validation loss: 0.01251\t accuracy: 0.614\n",
      "Epoch: 796\t Train loss: 0.004043\t accuracy: 0.8295\t Validation loss: 0.01308\t accuracy: 0.5876\n",
      "Epoch: 797\t Train loss: 0.004043\t accuracy: 0.8263\t Validation loss: 0.01315\t accuracy: 0.5876\n",
      "Epoch: 798\t Train loss: 0.004039\t accuracy: 0.8278\t Validation loss: 0.01302\t accuracy: 0.5925\n",
      "Epoch: 799\t Train loss: 0.004204\t accuracy: 0.8278\t Validation loss: 0.01267\t accuracy: 0.5925\n",
      "Epoch: 800\t Train loss: 0.003788\t accuracy: 0.8352\t Validation loss: 0.0127\t accuracy: 0.5905\n",
      "Epoch: 801\t Train loss: 0.004024\t accuracy: 0.8409\t Validation loss: 0.01286\t accuracy: 0.5945\n",
      "Epoch: 802\t Train loss: 0.003974\t accuracy: 0.8253\t Validation loss: 0.01292\t accuracy: 0.5842\n",
      "Epoch: 803\t Train loss: 0.003801\t accuracy: 0.8384\t Validation loss: 0.01268\t accuracy: 0.5959\n",
      "Epoch: 804\t Train loss: 0.003815\t accuracy: 0.8423\t Validation loss: 0.01286\t accuracy: 0.6004\n",
      "Epoch: 805\t Train loss: 0.004018\t accuracy: 0.8271\t Validation loss: 0.01283\t accuracy: 0.5935\n",
      "Epoch: 806\t Train loss: 0.003772\t accuracy: 0.837\t Validation loss: 0.01269\t accuracy: 0.6017\n",
      "Epoch: 807\t Train loss: 0.004022\t accuracy: 0.826\t Validation loss: 0.01279\t accuracy: 0.6004\n",
      "Epoch: 808\t Train loss: 0.004006\t accuracy: 0.8249\t Validation loss: 0.01268\t accuracy: 0.6077\n",
      "Epoch: 809\t Train loss: 0.003665\t accuracy: 0.8452\t Validation loss: 0.01286\t accuracy: 0.6097\n",
      "Epoch: 810\t Train loss: 0.003928\t accuracy: 0.8359\t Validation loss: 0.01299\t accuracy: 0.5959\n",
      "Epoch: 811\t Train loss: 0.003809\t accuracy: 0.8381\t Validation loss: 0.0131\t accuracy: 0.595\n",
      "Epoch: 812\t Train loss: 0.00393\t accuracy: 0.8313\t Validation loss: 0.01278\t accuracy: 0.592\n",
      "Epoch: 813\t Train loss: 0.004104\t accuracy: 0.8232\t Validation loss: 0.01288\t accuracy: 0.5891\n",
      "Epoch: 814\t Train loss: 0.00383\t accuracy: 0.8317\t Validation loss: 0.0127\t accuracy: 0.59\n",
      "Epoch: 815\t Train loss: 0.003868\t accuracy: 0.8352\t Validation loss: 0.01227\t accuracy: 0.6042\n",
      "Epoch: 816\t Train loss: 0.004045\t accuracy: 0.826\t Validation loss: 0.01267\t accuracy: 0.6082\n",
      "Epoch: 817\t Train loss: 0.003996\t accuracy: 0.8331\t Validation loss: 0.01274\t accuracy: 0.592\n",
      "Epoch: 818\t Train loss: 0.003794\t accuracy: 0.8406\t Validation loss: 0.01313\t accuracy: 0.5994\n",
      "Epoch: 819\t Train loss: 0.003662\t accuracy: 0.8473\t Validation loss: 0.01276\t accuracy: 0.5822\n",
      "Epoch: 820\t Train loss: 0.003922\t accuracy: 0.8335\t Validation loss: 0.01299\t accuracy: 0.5882\n",
      "Epoch: 821\t Train loss: 0.003833\t accuracy: 0.8366\t Validation loss: 0.01264\t accuracy: 0.589\n",
      "Epoch: 822\t Train loss: 0.004052\t accuracy: 0.8203\t Validation loss: 0.01246\t accuracy: 0.6121\n",
      "Epoch: 823\t Train loss: 0.003903\t accuracy: 0.8338\t Validation loss: 0.01275\t accuracy: 0.6047\n",
      "Epoch: 824\t Train loss: 0.003856\t accuracy: 0.8342\t Validation loss: 0.01296\t accuracy: 0.6083\n",
      "Epoch: 825\t Train loss: 0.003966\t accuracy: 0.8377\t Validation loss: 0.01288\t accuracy: 0.6072\n",
      "Epoch: 826\t Train loss: 0.00397\t accuracy: 0.8356\t Validation loss: 0.01244\t accuracy: 0.615\n",
      "Epoch: 827\t Train loss: 0.00389\t accuracy: 0.8331\t Validation loss: 0.01263\t accuracy: 0.6068\n",
      "Epoch: 828\t Train loss: 0.003951\t accuracy: 0.832\t Validation loss: 0.01293\t accuracy: 0.597\n",
      "Epoch: 829\t Train loss: 0.004133\t accuracy: 0.8232\t Validation loss: 0.01265\t accuracy: 0.6033\n",
      "Epoch: 830\t Train loss: 0.00399\t accuracy: 0.8313\t Validation loss: 0.01307\t accuracy: 0.6023\n",
      "Epoch: 831\t Train loss: 0.003955\t accuracy: 0.8281\t Validation loss: 0.01252\t accuracy: 0.6018\n",
      "Epoch: 832\t Train loss: 0.004042\t accuracy: 0.8306\t Validation loss: 0.01274\t accuracy: 0.6131\n",
      "Epoch: 833\t Train loss: 0.00371\t accuracy: 0.837\t Validation loss: 0.01287\t accuracy: 0.6009\n",
      "Epoch: 834\t Train loss: 0.003784\t accuracy: 0.8491\t Validation loss: 0.01282\t accuracy: 0.6013\n",
      "Epoch: 835\t Train loss: 0.003812\t accuracy: 0.8317\t Validation loss: 0.01295\t accuracy: 0.6037\n",
      "Epoch: 836\t Train loss: 0.003855\t accuracy: 0.8359\t Validation loss: 0.01256\t accuracy: 0.6209\n",
      "Epoch: 837\t Train loss: 0.00394\t accuracy: 0.8374\t Validation loss: 0.01268\t accuracy: 0.6155\n",
      "Epoch: 838\t Train loss: 0.003878\t accuracy: 0.8374\t Validation loss: 0.01293\t accuracy: 0.6032\n",
      "Epoch: 839\t Train loss: 0.003724\t accuracy: 0.8477\t Validation loss: 0.01298\t accuracy: 0.6047\n",
      "Epoch: 840\t Train loss: 0.003949\t accuracy: 0.8285\t Validation loss: 0.01281\t accuracy: 0.6126\n",
      "Epoch: 841\t Train loss: 0.004015\t accuracy: 0.8256\t Validation loss: 0.01322\t accuracy: 0.6062\n",
      "Epoch: 842\t Train loss: 0.004094\t accuracy: 0.8299\t Validation loss: 0.01276\t accuracy: 0.6052\n",
      "Epoch: 843\t Train loss: 0.00388\t accuracy: 0.8317\t Validation loss: 0.01283\t accuracy: 0.6038\n",
      "Epoch: 844\t Train loss: 0.003959\t accuracy: 0.8253\t Validation loss: 0.01302\t accuracy: 0.5978\n",
      "Epoch: 845\t Train loss: 0.003905\t accuracy: 0.8352\t Validation loss: 0.01248\t accuracy: 0.6018\n",
      "Epoch: 846\t Train loss: 0.00421\t accuracy: 0.8299\t Validation loss: 0.0127\t accuracy: 0.6013\n",
      "Epoch: 847\t Train loss: 0.003891\t accuracy: 0.8324\t Validation loss: 0.01298\t accuracy: 0.6096\n",
      "Epoch: 848\t Train loss: 0.00412\t accuracy: 0.8288\t Validation loss: 0.01266\t accuracy: 0.6141\n",
      "Epoch: 849\t Train loss: 0.003929\t accuracy: 0.8249\t Validation loss: 0.01273\t accuracy: 0.615\n",
      "Epoch: 850\t Train loss: 0.003912\t accuracy: 0.8288\t Validation loss: 0.01261\t accuracy: 0.5964\n",
      "Epoch: 851\t Train loss: 0.003905\t accuracy: 0.8295\t Validation loss: 0.01241\t accuracy: 0.6032\n",
      "Epoch: 852\t Train loss: 0.003941\t accuracy: 0.8384\t Validation loss: 0.01267\t accuracy: 0.6003\n",
      "Epoch: 853\t Train loss: 0.004111\t accuracy: 0.8214\t Validation loss: 0.01304\t accuracy: 0.5945\n",
      "Epoch: 854\t Train loss: 0.003885\t accuracy: 0.8374\t Validation loss: 0.01311\t accuracy: 0.6047\n",
      "Epoch: 855\t Train loss: 0.003993\t accuracy: 0.8196\t Validation loss: 0.01282\t accuracy: 0.6057\n",
      "Epoch: 856\t Train loss: 0.003873\t accuracy: 0.8391\t Validation loss: 0.01311\t accuracy: 0.594\n",
      "Epoch: 857\t Train loss: 0.003944\t accuracy: 0.8352\t Validation loss: 0.01273\t accuracy: 0.6067\n",
      "Epoch: 858\t Train loss: 0.004204\t accuracy: 0.8232\t Validation loss: 0.01259\t accuracy: 0.6008\n",
      "Epoch: 859\t Train loss: 0.003874\t accuracy: 0.8295\t Validation loss: 0.01276\t accuracy: 0.595\n",
      "Epoch: 860\t Train loss: 0.004119\t accuracy: 0.8303\t Validation loss: 0.01282\t accuracy: 0.5852\n",
      "Epoch: 861\t Train loss: 0.003873\t accuracy: 0.8349\t Validation loss: 0.01272\t accuracy: 0.6101\n",
      "Epoch: 862\t Train loss: 0.004098\t accuracy: 0.8342\t Validation loss: 0.01264\t accuracy: 0.6047\n",
      "Epoch: 863\t Train loss: 0.003826\t accuracy: 0.8366\t Validation loss: 0.01284\t accuracy: 0.6018\n",
      "Epoch: 864\t Train loss: 0.003993\t accuracy: 0.8349\t Validation loss: 0.01245\t accuracy: 0.6125\n",
      "Epoch: 865\t Train loss: 0.003824\t accuracy: 0.8413\t Validation loss: 0.01288\t accuracy: 0.6091\n",
      "Epoch: 866\t Train loss: 0.003899\t accuracy: 0.8327\t Validation loss: 0.01273\t accuracy: 0.5891\n",
      "Epoch: 867\t Train loss: 0.004102\t accuracy: 0.8292\t Validation loss: 0.01287\t accuracy: 0.6028\n",
      "Epoch: 868\t Train loss: 0.003864\t accuracy: 0.8366\t Validation loss: 0.01265\t accuracy: 0.5959\n",
      "Epoch: 869\t Train loss: 0.003795\t accuracy: 0.8402\t Validation loss: 0.01273\t accuracy: 0.6013\n",
      "Epoch: 870\t Train loss: 0.003821\t accuracy: 0.8374\t Validation loss: 0.01249\t accuracy: 0.6115\n",
      "Epoch: 871\t Train loss: 0.003832\t accuracy: 0.8356\t Validation loss: 0.01247\t accuracy: 0.5964\n",
      "Epoch: 872\t Train loss: 0.003797\t accuracy: 0.8438\t Validation loss: 0.01301\t accuracy: 0.6082\n",
      "Epoch: 873\t Train loss: 0.00375\t accuracy: 0.843\t Validation loss: 0.01273\t accuracy: 0.6086\n",
      "Epoch: 874\t Train loss: 0.003938\t accuracy: 0.8256\t Validation loss: 0.01285\t accuracy: 0.5896\n",
      "Epoch: 875\t Train loss: 0.004037\t accuracy: 0.8278\t Validation loss: 0.01278\t accuracy: 0.5989\n",
      "Epoch: 876\t Train loss: 0.00413\t accuracy: 0.8242\t Validation loss: 0.01303\t accuracy: 0.6087\n",
      "Epoch: 877\t Train loss: 0.003645\t accuracy: 0.8498\t Validation loss: 0.01244\t accuracy: 0.6203\n",
      "Epoch: 878\t Train loss: 0.003923\t accuracy: 0.8327\t Validation loss: 0.01259\t accuracy: 0.6081\n",
      "Epoch: 879\t Train loss: 0.003777\t accuracy: 0.8366\t Validation loss: 0.01264\t accuracy: 0.6067\n",
      "Epoch: 880\t Train loss: 0.003947\t accuracy: 0.8263\t Validation loss: 0.01258\t accuracy: 0.6091\n",
      "Epoch: 881\t Train loss: 0.003611\t accuracy: 0.8477\t Validation loss: 0.01245\t accuracy: 0.6145\n",
      "Epoch: 882\t Train loss: 0.003701\t accuracy: 0.8381\t Validation loss: 0.01259\t accuracy: 0.6067\n",
      "Epoch: 883\t Train loss: 0.004049\t accuracy: 0.8313\t Validation loss: 0.01255\t accuracy: 0.6165\n",
      "Epoch: 884\t Train loss: 0.004067\t accuracy: 0.8249\t Validation loss: 0.01242\t accuracy: 0.619\n",
      "Epoch: 885\t Train loss: 0.003756\t accuracy: 0.8413\t Validation loss: 0.01302\t accuracy: 0.6048\n",
      "Epoch: 886\t Train loss: 0.00381\t accuracy: 0.8455\t Validation loss: 0.01275\t accuracy: 0.6028\n",
      "Epoch: 887\t Train loss: 0.004117\t accuracy: 0.8232\t Validation loss: 0.01256\t accuracy: 0.6057\n",
      "Epoch: 888\t Train loss: 0.003752\t accuracy: 0.842\t Validation loss: 0.01261\t accuracy: 0.6238\n",
      "Epoch: 889\t Train loss: 0.003715\t accuracy: 0.8398\t Validation loss: 0.01276\t accuracy: 0.6077\n",
      "Epoch: 890\t Train loss: 0.003954\t accuracy: 0.8327\t Validation loss: 0.0124\t accuracy: 0.6234\n",
      "Epoch: 891\t Train loss: 0.003914\t accuracy: 0.8374\t Validation loss: 0.01235\t accuracy: 0.619\n",
      "Epoch: 892\t Train loss: 0.003903\t accuracy: 0.8349\t Validation loss: 0.01258\t accuracy: 0.6184\n",
      "Epoch: 893\t Train loss: 0.003699\t accuracy: 0.842\t Validation loss: 0.01284\t accuracy: 0.6013\n",
      "Epoch: 894\t Train loss: 0.003977\t accuracy: 0.8345\t Validation loss: 0.01274\t accuracy: 0.619\n",
      "Epoch: 895\t Train loss: 0.00366\t accuracy: 0.8445\t Validation loss: 0.01261\t accuracy: 0.6229\n",
      "Epoch: 896\t Train loss: 0.003943\t accuracy: 0.8345\t Validation loss: 0.01241\t accuracy: 0.6224\n",
      "Epoch: 897\t Train loss: 0.003945\t accuracy: 0.8299\t Validation loss: 0.01258\t accuracy: 0.6116\n",
      "Epoch: 898\t Train loss: 0.003738\t accuracy: 0.8484\t Validation loss: 0.01298\t accuracy: 0.5978\n",
      "Epoch: 899\t Train loss: 0.003818\t accuracy: 0.8356\t Validation loss: 0.01257\t accuracy: 0.6125\n",
      "Epoch: 900\t Train loss: 0.003766\t accuracy: 0.8374\t Validation loss: 0.01252\t accuracy: 0.5876\n",
      "Epoch: 901\t Train loss: 0.003695\t accuracy: 0.8388\t Validation loss: 0.01242\t accuracy: 0.5895\n",
      "Epoch: 902\t Train loss: 0.003891\t accuracy: 0.8306\t Validation loss: 0.01252\t accuracy: 0.6042\n",
      "Epoch: 903\t Train loss: 0.003797\t accuracy: 0.8402\t Validation loss: 0.01264\t accuracy: 0.613\n",
      "Epoch: 904\t Train loss: 0.003797\t accuracy: 0.8327\t Validation loss: 0.01267\t accuracy: 0.6116\n",
      "Epoch: 905\t Train loss: 0.003872\t accuracy: 0.8313\t Validation loss: 0.01256\t accuracy: 0.6008\n",
      "Epoch: 906\t Train loss: 0.004022\t accuracy: 0.8313\t Validation loss: 0.01276\t accuracy: 0.6052\n",
      "Epoch: 907\t Train loss: 0.004177\t accuracy: 0.8246\t Validation loss: 0.01259\t accuracy: 0.6037\n",
      "Epoch: 908\t Train loss: 0.003864\t accuracy: 0.8342\t Validation loss: 0.01237\t accuracy: 0.5994\n",
      "Epoch: 909\t Train loss: 0.00386\t accuracy: 0.8388\t Validation loss: 0.01291\t accuracy: 0.5921\n",
      "Epoch: 910\t Train loss: 0.003716\t accuracy: 0.8406\t Validation loss: 0.01245\t accuracy: 0.614\n",
      "Epoch: 911\t Train loss: 0.003927\t accuracy: 0.8356\t Validation loss: 0.01257\t accuracy: 0.6111\n",
      "Epoch: 912\t Train loss: 0.00379\t accuracy: 0.8438\t Validation loss: 0.01262\t accuracy: 0.6193\n",
      "Epoch: 913\t Train loss: 0.003602\t accuracy: 0.8505\t Validation loss: 0.01258\t accuracy: 0.6052\n",
      "Epoch: 914\t Train loss: 0.00372\t accuracy: 0.8487\t Validation loss: 0.01262\t accuracy: 0.6052\n",
      "Epoch: 915\t Train loss: 0.00401\t accuracy: 0.8317\t Validation loss: 0.01289\t accuracy: 0.6072\n",
      "Epoch: 916\t Train loss: 0.003893\t accuracy: 0.8299\t Validation loss: 0.01255\t accuracy: 0.618\n",
      "Epoch: 917\t Train loss: 0.003749\t accuracy: 0.8345\t Validation loss: 0.01257\t accuracy: 0.6125\n",
      "Epoch: 918\t Train loss: 0.003733\t accuracy: 0.8366\t Validation loss: 0.01269\t accuracy: 0.6052\n",
      "Epoch: 919\t Train loss: 0.003836\t accuracy: 0.8366\t Validation loss: 0.01302\t accuracy: 0.5968\n",
      "Epoch: 920\t Train loss: 0.003926\t accuracy: 0.8352\t Validation loss: 0.01318\t accuracy: 0.6061\n",
      "Epoch: 921\t Train loss: 0.003959\t accuracy: 0.8352\t Validation loss: 0.01276\t accuracy: 0.6061\n",
      "Epoch: 922\t Train loss: 0.003721\t accuracy: 0.8413\t Validation loss: 0.01319\t accuracy: 0.6037\n",
      "Epoch: 923\t Train loss: 0.003912\t accuracy: 0.8338\t Validation loss: 0.01286\t accuracy: 0.6051\n",
      "Epoch: 924\t Train loss: 0.003764\t accuracy: 0.8395\t Validation loss: 0.01269\t accuracy: 0.6105\n",
      "Epoch: 925\t Train loss: 0.003788\t accuracy: 0.8438\t Validation loss: 0.0124\t accuracy: 0.6111\n",
      "Epoch: 926\t Train loss: 0.003914\t accuracy: 0.8398\t Validation loss: 0.0128\t accuracy: 0.618\n",
      "Epoch: 927\t Train loss: 0.003901\t accuracy: 0.8416\t Validation loss: 0.01271\t accuracy: 0.6081\n",
      "Epoch: 928\t Train loss: 0.003816\t accuracy: 0.8345\t Validation loss: 0.01317\t accuracy: 0.5974\n",
      "Epoch: 929\t Train loss: 0.003837\t accuracy: 0.8416\t Validation loss: 0.01231\t accuracy: 0.6219\n",
      "Epoch: 930\t Train loss: 0.003809\t accuracy: 0.8423\t Validation loss: 0.01257\t accuracy: 0.6082\n",
      "Epoch: 931\t Train loss: 0.003972\t accuracy: 0.8303\t Validation loss: 0.01228\t accuracy: 0.6014\n",
      "Epoch: 932\t Train loss: 0.003892\t accuracy: 0.8359\t Validation loss: 0.01264\t accuracy: 0.5993\n",
      "Epoch: 933\t Train loss: 0.003651\t accuracy: 0.8469\t Validation loss: 0.01245\t accuracy: 0.6097\n",
      "Epoch: 934\t Train loss: 0.003896\t accuracy: 0.8448\t Validation loss: 0.01266\t accuracy: 0.6008\n",
      "Epoch: 935\t Train loss: 0.00375\t accuracy: 0.8377\t Validation loss: 0.01241\t accuracy: 0.616\n",
      "Epoch: 936\t Train loss: 0.003738\t accuracy: 0.8423\t Validation loss: 0.01269\t accuracy: 0.6057\n",
      "Epoch: 937\t Train loss: 0.003956\t accuracy: 0.8352\t Validation loss: 0.01253\t accuracy: 0.616\n",
      "Epoch: 938\t Train loss: 0.00366\t accuracy: 0.8466\t Validation loss: 0.01249\t accuracy: 0.6136\n",
      "Epoch: 939\t Train loss: 0.003602\t accuracy: 0.8505\t Validation loss: 0.01248\t accuracy: 0.6057\n",
      "Epoch: 940\t Train loss: 0.003792\t accuracy: 0.8363\t Validation loss: 0.01244\t accuracy: 0.6121\n",
      "Epoch: 941\t Train loss: 0.003625\t accuracy: 0.8423\t Validation loss: 0.01276\t accuracy: 0.596\n",
      "Epoch: 942\t Train loss: 0.003633\t accuracy: 0.8462\t Validation loss: 0.01248\t accuracy: 0.6121\n",
      "Epoch: 943\t Train loss: 0.003701\t accuracy: 0.8427\t Validation loss: 0.0123\t accuracy: 0.6003\n",
      "Epoch: 944\t Train loss: 0.003774\t accuracy: 0.8395\t Validation loss: 0.01232\t accuracy: 0.6135\n",
      "Epoch: 945\t Train loss: 0.003831\t accuracy: 0.8409\t Validation loss: 0.01269\t accuracy: 0.5959\n",
      "Epoch: 946\t Train loss: 0.004003\t accuracy: 0.8338\t Validation loss: 0.01262\t accuracy: 0.6052\n",
      "Epoch: 947\t Train loss: 0.003817\t accuracy: 0.8335\t Validation loss: 0.01266\t accuracy: 0.6043\n",
      "Epoch: 948\t Train loss: 0.003634\t accuracy: 0.8438\t Validation loss: 0.01263\t accuracy: 0.6032\n",
      "Epoch: 949\t Train loss: 0.003944\t accuracy: 0.8285\t Validation loss: 0.01255\t accuracy: 0.5964\n",
      "Epoch: 950\t Train loss: 0.003697\t accuracy: 0.8487\t Validation loss: 0.01245\t accuracy: 0.5993\n",
      "Epoch: 951\t Train loss: 0.003858\t accuracy: 0.8299\t Validation loss: 0.01263\t accuracy: 0.5964\n",
      "Epoch: 952\t Train loss: 0.003599\t accuracy: 0.8551\t Validation loss: 0.01256\t accuracy: 0.6106\n",
      "Epoch: 953\t Train loss: 0.003883\t accuracy: 0.8335\t Validation loss: 0.01248\t accuracy: 0.6179\n",
      "Epoch: 954\t Train loss: 0.003731\t accuracy: 0.8434\t Validation loss: 0.01263\t accuracy: 0.6017\n",
      "Epoch: 955\t Train loss: 0.003529\t accuracy: 0.8427\t Validation loss: 0.01308\t accuracy: 0.5988\n",
      "Epoch: 956\t Train loss: 0.003728\t accuracy: 0.8342\t Validation loss: 0.01264\t accuracy: 0.616\n",
      "Epoch: 957\t Train loss: 0.003778\t accuracy: 0.842\t Validation loss: 0.01243\t accuracy: 0.6082\n",
      "Epoch: 958\t Train loss: 0.003694\t accuracy: 0.8494\t Validation loss: 0.01254\t accuracy: 0.6072\n",
      "Epoch: 959\t Train loss: 0.003736\t accuracy: 0.8398\t Validation loss: 0.01292\t accuracy: 0.5978\n",
      "Epoch: 960\t Train loss: 0.003867\t accuracy: 0.826\t Validation loss: 0.01266\t accuracy: 0.6101\n",
      "Epoch: 961\t Train loss: 0.003845\t accuracy: 0.8366\t Validation loss: 0.01235\t accuracy: 0.616\n",
      "Epoch: 962\t Train loss: 0.003697\t accuracy: 0.8438\t Validation loss: 0.01232\t accuracy: 0.6248\n",
      "Epoch: 963\t Train loss: 0.003761\t accuracy: 0.8441\t Validation loss: 0.01258\t accuracy: 0.6062\n",
      "Epoch: 964\t Train loss: 0.003619\t accuracy: 0.8526\t Validation loss: 0.01246\t accuracy: 0.6067\n",
      "Epoch: 965\t Train loss: 0.003767\t accuracy: 0.8512\t Validation loss: 0.01276\t accuracy: 0.6013\n",
      "Epoch: 966\t Train loss: 0.003723\t accuracy: 0.8416\t Validation loss: 0.01234\t accuracy: 0.6023\n",
      "Epoch: 967\t Train loss: 0.003564\t accuracy: 0.853\t Validation loss: 0.01286\t accuracy: 0.5965\n",
      "Epoch: 968\t Train loss: 0.00384\t accuracy: 0.8377\t Validation loss: 0.01252\t accuracy: 0.6077\n",
      "Epoch: 969\t Train loss: 0.00384\t accuracy: 0.8342\t Validation loss: 0.01246\t accuracy: 0.6044\n",
      "Epoch: 970\t Train loss: 0.003699\t accuracy: 0.8448\t Validation loss: 0.0124\t accuracy: 0.6145\n",
      "Epoch: 971\t Train loss: 0.003797\t accuracy: 0.8349\t Validation loss: 0.01277\t accuracy: 0.588\n",
      "Epoch: 972\t Train loss: 0.003772\t accuracy: 0.8366\t Validation loss: 0.01235\t accuracy: 0.6082\n",
      "Epoch: 973\t Train loss: 0.0036\t accuracy: 0.848\t Validation loss: 0.01224\t accuracy: 0.6234\n",
      "Epoch: 974\t Train loss: 0.003689\t accuracy: 0.8377\t Validation loss: 0.01262\t accuracy: 0.6028\n",
      "Epoch: 975\t Train loss: 0.003924\t accuracy: 0.8349\t Validation loss: 0.01295\t accuracy: 0.599\n",
      "Epoch: 976\t Train loss: 0.003511\t accuracy: 0.8501\t Validation loss: 0.01215\t accuracy: 0.6058\n",
      "Epoch: 977\t Train loss: 0.003781\t accuracy: 0.8413\t Validation loss: 0.01237\t accuracy: 0.6004\n",
      "Epoch: 978\t Train loss: 0.003802\t accuracy: 0.8327\t Validation loss: 0.01226\t accuracy: 0.5965\n",
      "Epoch: 979\t Train loss: 0.003686\t accuracy: 0.8487\t Validation loss: 0.01251\t accuracy: 0.6136\n",
      "Epoch: 980\t Train loss: 0.003749\t accuracy: 0.8377\t Validation loss: 0.01278\t accuracy: 0.6072\n",
      "Epoch: 981\t Train loss: 0.003829\t accuracy: 0.8398\t Validation loss: 0.01247\t accuracy: 0.6184\n",
      "Epoch: 982\t Train loss: 0.003912\t accuracy: 0.8303\t Validation loss: 0.01267\t accuracy: 0.6014\n",
      "Epoch: 983\t Train loss: 0.003941\t accuracy: 0.8214\t Validation loss: 0.01244\t accuracy: 0.6077\n",
      "Epoch: 984\t Train loss: 0.003902\t accuracy: 0.8359\t Validation loss: 0.01226\t accuracy: 0.613\n",
      "Epoch: 985\t Train loss: 0.003628\t accuracy: 0.8459\t Validation loss: 0.01243\t accuracy: 0.615\n",
      "Epoch: 986\t Train loss: 0.003657\t accuracy: 0.8377\t Validation loss: 0.01261\t accuracy: 0.6048\n",
      "Epoch: 987\t Train loss: 0.003859\t accuracy: 0.8374\t Validation loss: 0.01214\t accuracy: 0.6209\n",
      "Epoch: 988\t Train loss: 0.003594\t accuracy: 0.8516\t Validation loss: 0.01223\t accuracy: 0.6053\n",
      "Epoch: 989\t Train loss: 0.003791\t accuracy: 0.8452\t Validation loss: 0.01244\t accuracy: 0.616\n",
      "Epoch: 990\t Train loss: 0.003656\t accuracy: 0.8501\t Validation loss: 0.01266\t accuracy: 0.6307\n",
      "Epoch: 991\t Train loss: 0.003625\t accuracy: 0.8441\t Validation loss: 0.01285\t accuracy: 0.6072\n",
      "Epoch: 992\t Train loss: 0.003795\t accuracy: 0.8441\t Validation loss: 0.01252\t accuracy: 0.6174\n",
      "Epoch: 993\t Train loss: 0.003798\t accuracy: 0.8352\t Validation loss: 0.01243\t accuracy: 0.6223\n",
      "Epoch: 994\t Train loss: 0.003776\t accuracy: 0.8384\t Validation loss: 0.01224\t accuracy: 0.6126\n",
      "Epoch: 995\t Train loss: 0.003845\t accuracy: 0.8438\t Validation loss: 0.01243\t accuracy: 0.6307\n",
      "Epoch: 996\t Train loss: 0.003758\t accuracy: 0.842\t Validation loss: 0.01279\t accuracy: 0.5979\n",
      "Epoch: 997\t Train loss: 0.003701\t accuracy: 0.8416\t Validation loss: 0.01239\t accuracy: 0.6179\n",
      "Epoch: 998\t Train loss: 0.003746\t accuracy: 0.8406\t Validation loss: 0.01241\t accuracy: 0.6263\n",
      "Epoch: 999\t Train loss: 0.003709\t accuracy: 0.8398\t Validation loss: 0.01269\t accuracy: 0.6043\n",
      "Test loss: 0.01225\t accuracy: 0.6277\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "IN_SIZE = 11\n",
    "OUT_SIZE = 1\n",
    "LAYERS = [512, 0.5, 256]\n",
    "ACTIVATION = F.relu\n",
    "OUTPUT_FUNC = None\n",
    "LR = 0.003\n",
    "EPOCHS = 1000\n",
    "EARLY_STOPPING = 0\n",
    "\n",
    "model = SimpleMultiLayerNet(\n",
    "    input_size=IN_SIZE,\n",
    "    output_size=OUT_SIZE,\n",
    "    activation=ACTIVATION,\n",
    "    output_func=OUTPUT_FUNC,\n",
    "    layers=LAYERS\n",
    ")\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optim=optimizer,\n",
    "    loss=loss,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    metric=mse_accuracy\n",
    ")\n",
    "\n",
    "trainer.train(epochs=EPOCHS, early_stoping=EARLY_STOPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1e8513bb80>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = [train_loss for train_loss, train_metric, valid_loss, valid_metric in trainer.history]\n",
    "valid_loss = [valid_loss for train_loss, train_metric, valid_loss, valid_metric in trainer.history]\n",
    "train_acc = [train_metric for train_loss, train_metric, valid_loss, valid_metric in trainer.history]\n",
    "valid_acc = [valid_metric for train_loss, train_metric, valid_loss, valid_metric in trainer.history]\n",
    "\n",
    "# plt.plot(range(len(train_loss)), train_loss)\n",
    "# plt.plot(range(len(train_acc)), train_acc)\n",
    "plt.plot(range(len(valid_loss)), valid_loss)\n",
    "# plt.plot(range(len(valid_acc)), valid_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ssne')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a38050f8ca24262ead381cbc68a5f26ee5ce1e0f01d0472f124efe95ebd2613"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
